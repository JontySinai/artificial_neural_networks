{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Preprocessing in Pytorch\n",
    "\n",
    "**|| Jonty Sinai ||** 28-04-2019\n",
    "\n",
    "In this notebook I show how a **translation dataset** can be preprocessed into pairs of input and target sequences for training a sequence-to-sequence model with PyTorch. This dataset will then be used to test the implementation of two of the original sequence-to-sequence architectures introduced in [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) (Ilya Sutskever et al 2014) and [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)(Kyunghyun Cho et al 2014) respectively. \n",
    "\n",
    "The dataset which I will use is a simplified English-French translation dataset based on the [official PyTorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py). Whereas in my [ResNet implementation](https://github.com/JontySinai/artificial_neural_networks/blob/master/notebooks/resnet.ipynb) the goal was to explore modular composeability with PyTorch, the goal here is to explore sequence-to-sequence design patterns and engineering with PyTorch.\n",
    "\n",
    "> **Note:** Compared to computer vision, feature selection and data preprocessing in natural language processing (NLP) requires more care and attention. There is no correct way of preprocessing text, although there are many incorrect ways. Choices must be made carefully and efficiently. As a result, a good portion of this notebook is spent preprocessing the dataset and converting it into the right format for sequence-to-sequence learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8145d8a6d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "from io import open\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HOME = os.environ['AI_HOME']\n",
    "ROOT = os.path.join(HOME, 'artificial_neural_networks')\n",
    "DATA = os.path.join(ROOT, 'data')\n",
    "ENG_FR = os.path.join(DATA, 'english_french')\n",
    "\n",
    "random.seed(1901)\n",
    "np.random.seed(1901)\n",
    "torch.manual_seed(1901)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "English-French sentence pairs can be downloaded from the PyTorch translation tutorial [here](https://download.pytorch.org/tutorial/data.zip) (the original data comes from [https://tatoeba.org/eng/downloads](https://tatoeba.org/eng/downloads) and has been paired thanks to [https://www.manythings.org/anki/](https://www.manythings.org/anki/)). \n",
    "\n",
    "We will need to do the hardwork of preprocessing this data into tensors which we can feed into our model. We'll start by exploring the dataset and then understanding the I/O requirements of the model architecture. This will inform our choices for preprocessing. Since this is a sequence-to-sequence model, we'll need to be careful to ensure that pairs of input-output sequences are aligned during training and evaluation.\n",
    "\n",
    "### Data Preview\n",
    "\n",
    "The data is stored on one file with each line representing an English-French sentence pair. Each English and French sentence is tab separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(ENG_FR, 'eng-fra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Wow!\tÇa alors !\n",
      "Fire!\tAu feu !\n",
      "Help!\tÀ l'aide !\n",
      "Jump.\tSaute.\n",
      "Stop!\tÇa suffit !\n",
      "Stop!\tStop !\n",
      "Stop!\tArrête-toi !\n"
     ]
    }
   ],
   "source": [
    "raw_pairs = open(data_path, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "for line in raw_pairs[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Normalisation\n",
    "\n",
    "As you can see there are some Unicode characters such as \"À\" and \"Ç\". Raw Unicode can be problematic for deep learning as seemingly identical characters on screen can have different representations on disk. For this reason we will need to _normalise_ Unicode characters so that representation is consistent. For more detail on why Unicode normalisation is important see this blogpost: [When \"Zoë\" !== \"Zoë\". Or why you need to normalize Unicode strings](https://withblue.ink/2019/03/11/why-you-need-to-normalize-unicode-strings.html) by [Alessandro Segala](https://withblue.ink/about.html). \n",
    "\n",
    "Unicode characters can be normalised by converting them into a so-called \"canonical form\" of which there are four. We will used **NFD** or [Unicode Normal Form Decomposition](https://unicode.org/reports/tr15/#Norm_Forms) to decompose characters with accents. For example \"À\" will be decomposed into \"A\" and \"̖\". We will then have to remove the accent using a filter on so-called _non-spacing mark_ characters. The solution is taken from the PyTorch tutorial (originally StackOverflow) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_normalisation(s):\n",
    "    # ref: https://stackoverflow.com/a/518232/2809427\n",
    "    return ''.join(\n",
    "        char for char in unicodedata.normalize('NFD', s)  # \"À\" -->  \"A\" + \"̖\"\n",
    "        if unicodedata.category(char) != 'Mn')  # remove \"̖\"s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Wow!\tCa alors !\n",
      "Fire!\tAu feu !\n",
      "Help!\tA l'aide !\n",
      "Jump.\tSaute.\n",
      "Stop!\tCa suffit !\n",
      "Stop!\tStop !\n",
      "Stop!\tArrete-toi !\n"
     ]
    }
   ],
   "source": [
    "for line in raw_pairs[:10]:\n",
    "    print(unicode_normalisation(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then remove all non-letter characters such as numbers and punctuation, keeping only the end of sentence markers \".\", \"!\", \"?\", which we will add a space in front of so we can treat them as separate tokens. As part of standard string normalisation for NLP we will convert all characters to lowercase and strip whitespace. \n",
    "\n",
    ">**Note:** as a result words like \"aren't\" will become \"aren\" \"t\" and will be treated as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_string(s):\n",
    "    s = unicode_normalisation(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)  # add space in front of \".\", \"!\" \"?\"\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # remove unwanted characets\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . va !\n",
      "run ! cours !\n",
      "run ! courez !\n",
      "wow ! ca alors !\n",
      "fire ! au feu !\n",
      "help ! a l aide !\n",
      "jump . saute .\n",
      "stop ! ca suffit !\n",
      "stop ! stop !\n",
      "stop ! arrete toi !\n"
     ]
    }
   ],
   "source": [
    "for line in raw_pairs[:10]:\n",
    "    print(normalise_string(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "The next step is to tokenise each sentence. Tokenisation is useful because it reduces sentences (which occur uniquely in the dataset) into smaller units. We can then represent the _feature space_ of each language in terms of these tokens. This feature space is orders of magnitude smaller than the sentence space (cardinality ~ dataset) and allows us to represent sentences in terms of shared features. Finally we can treat each _timestep_ in each sentence as a token. \n",
    "\n",
    "> Thus we can represent each sentence as a **sequence of tokens**.\n",
    "\n",
    "One important aspect of tokenisation is that we can _index each unique token_ for each language to form what is known as **vocabulary**, which is _1-to-1_ mapping between indices and tokens.\n",
    "\n",
    "An example of a vocabulary mapping, containing two special tokens `<SOS>` and `<EOS>` start/end of sequence markers, is as follows:\n",
    "\n",
    "<img src=\"assets/word-encoding.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(sentence):\n",
    "    return sentence.split(' ')\n",
    "\n",
    "def tokenise_pair(pair):\n",
    "    return [tokenise(normalise_string(s)) for s in pair.split('\\t')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '.'] \t ['va', '!']\n",
      "['run', '!'] \t ['cours', '!']\n",
      "['run', '!'] \t ['courez', '!']\n",
      "['wow', '!'] \t ['ca', 'alors', '!']\n",
      "['fire', '!'] \t ['au', 'feu', '!']\n"
     ]
    }
   ],
   "source": [
    "for line in raw_pairs[:5]:\n",
    "    print(tokenise_pair(line)[0], '\\t', tokenise_pair(line)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence in each pair is now a sequence of tokens.\n",
    "\n",
    "#### Remove Low Frequency Tokens\n",
    "\n",
    "Those familiar with standard NLP preprocessing will know that the tokenisation process is typically followed by the removal of stopwords (such as \"the\", \"a\"), pronouns (such as \"they\") and lemmatisation (\"going\" --> \"go\"). For neural machine translation we do want to keep stopwords, pronouns and tense as these are important for accurate translation. \n",
    "\n",
    "One of the reasons why such filtering of tokens is important is to reduce the size of the vocabulary - as we will see below, this will reduce complexity when computing vector representations. However if stopwords and lemmatisation is important for translation, then how can we reduce the size of the vocabulary? One way is to remove tokens with relatively low frequency. For example, if some of our training pairs contained references to specific names or terms, then those tokens will be noise in the translation task. While it is hard to identify all such words, it is safe to assume that they occur with low frequency.\n",
    "\n",
    "In fact, term frequencies tend to be long tailed, so we can significantly reduce the vocabulary dimension by removing all tokens with very low frequency.\n",
    "\n",
    "To see this, let's go through all the English tokens and plots its distribution by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022097\n"
     ]
    }
   ],
   "source": [
    "en_tokens = []\n",
    "for line in raw_pairs:\n",
    "    en_tokens.extend(tokenise_pair(line)[0])\n",
    "    \n",
    "print(len(en_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 1 022 097 English tokens in total. We can count the unique number of tokens and their frequencies using Python's `Counter` container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13041\n"
     ]
    }
   ],
   "source": [
    "en_term_frequencies = Counter(en_tokens)\n",
    "\n",
    "print(len(en_term_frequencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total there are 13 041 tokens. Let's see what the the distribution of their frequencies are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([freq for _, freq in en_term_frequencies.items()], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most of the tokens have a similar frequency with a few of them having very high frequency (for example, stop words). We can inspect the mid to low frequency distribution by clipping the high frequency tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([freq for _, freq in en_term_frequencies.items() if freq < 100], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there is a very high number of tokens with very low frequency. These are the ones which we will wish to prune. We can prune tokens by removing the highest (lowest) tail of the terms sorted by frequency in descending (ascending) order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "descending_order_percentile = 99\n",
    "ascending_order_percentile = 100 - descending_order_percentile\n",
    "\n",
    "# np.percentile sorts in ascending order by default\n",
    "freq_threshold = np.percentile([freq for _, freq in en_term_frequencies.items()], ascending_order_percentile)\n",
    "\n",
    "print(freq_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove all tokens with frequency less or equal to 1 and reduce dimensionality of the vocabulary by a factor in the order of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8910\n"
     ]
    }
   ],
   "source": [
    "keep_tokens = []\n",
    "for term, freq in en_term_frequencies.items():\n",
    "    if freq > freq_threshold:\n",
    "        keep_tokens.append(term)\n",
    "        \n",
    "print(len(keep_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** In the vocabulary class which we'll create below we'll use `defaultdict` to count term frequencies one at a time rather than `Counter`. This is because we will create the vocabulary as we encounter terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onehot-Vectors\n",
    "\n",
    "Typicall we convert each token into a onehot-vector of dimensionality $\\mathbb{R}^{\\lvert V \\rvert}$ where $\\lvert V \\rvert$ is the vocabulary size and each direction is a unique token.\n",
    "\n",
    "In the following example, taken from Andrew Ng's Deep Learning Course, an example English vocabulary could contain 10 000 unique tokens, including a special `\"<UNK>\"` token to represent unknown tokens. If **\"queen\"** has index **7157** then it will have a $1$ at the 7157<sup>th</sup> direction in $\\mathbb{R}^{\\lvert V \\rvert}$ and $0$ everywhere else.\n",
    "\n",
    "<img src=\"assets/onehot-vectors.png\" width=\"600\">\n",
    "<br/>\n",
    "\n",
    "#### Continuous Vector Embeddings\n",
    "\n",
    "The problem with onehot vector representations is that they encode information inefficiently. In a vector of length 10,000, 9,999 directions contain zero information content and 1 direction contains 100% of the information content! A well studied way of encoding information more efficiently is to _compress_ the _sparse_ onehot vector into a _dense_ or _continuous_ vector representation with smaller dimensionality. We lose the meaning of each direction corresponding to a unique vector but we gain a memory cost saving. \n",
    "\n",
    "> Even more than that, there is a whole line of research studying continuous vector embeddings where the goal is to learn meaningful directions in an unsupervised way. The most famous of which are [word2vec](https://arxiv.org/abs/1301.3781), [fastText](https://arxiv.org/abs/1607.01759) and [GloVe](https://nlp.stanford.edu/projects/glove/). The encoding efficiency of these embeddings have been shown to improve downstream tasks such as sequence-to-sequence modelling. \n",
    "\n",
    "Using such pre-trained embeddings, words such as **\"queen\"** can now be decomposed into a smaller, continuous vector where certain directions \\*may have certain encoded meanings. \n",
    "\n",
    "<img src=\"assets/embedding-vectors.png\" width=\"600\">\n",
    "<br/>\n",
    "\n",
    "### Sequence Encoding\n",
    "\n",
    "Since we will typically pass onehot vectors through an initial **embedding layer (transformation)**, we do not need to represent each token as a onehot vector in PyTorch. This is because under the hood, PyTorch will precompute the embeddings for each unique token and use a lookup-table to map each token to its embedding vector. Thus we can instead just use the token index at each timestep so that the word **\"queen\"** occuring at time $t$ will simply be represented by **7157** at time $t$.\n",
    "\n",
    ">Thus by indexing tokens uniquely for each language we can represent each sentence as a **sequence of indices**.\n",
    "\n",
    "We will manage tokenisation, indexing and sequence conversion using a **Vocabulary** class. This class will be responsible for:\n",
    "\n",
    "- Mapping tokens to indices and vice-versa.\n",
    "    - This will include the three special tokens mentioned earlier, namely `<SOS>`, `<EOS>` and `<UNK>`.\n",
    "- Adding tokens to the vocabulary.\n",
    "- Keeping track of vocabulary size.\n",
    "\n",
    "We will then use this class to convert **sequences of tokens** into **sequences of indices**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "class Vocab:\n",
    "    \n",
    "    def __init__(self, name: str, tokens: list, prune_at: int):\n",
    "        self.name = name\n",
    "        self.index2token = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.token2index = defaultdict(lambda: UNK_TOKEN,  # return UNK_TOKEN if unknown token encountered\n",
    "                                       {\"<pad>\": PAD_TOKEN, \"<sos>\": SOS_TOKEN, \"<eos>\": EOS_TOKEN})  # init key-vals\n",
    "        \n",
    "        self.num_pruned = 0\n",
    "        \n",
    "        self._add_tokens(tokens, prune_at)\n",
    "        \n",
    "    def _add_tokens(self, tokens: list, descending_percentile: int):\n",
    "        \"\"\"\n",
    "            tokens (list): list of non-unique tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        self.term_frequencies = Counter(tokens)\n",
    "        \n",
    "        # np.percentile sorts an array in ascending order so if we wish to prune \n",
    "        # tokens with term frequencies in the lowest 99th percentile, we can calculate \n",
    "        # the threshold at the 1st percentile in ascending order.\n",
    "        ascending_percentile = 100 - descending_percentile\n",
    "        freq_threshold = np.percentile(\n",
    "            [[freq for freq in self.term_frequencies.values()]], ascending_percentile)\n",
    "        \n",
    "        for term, freq in self.term_frequencies.items():\n",
    "            if freq >= freq_threshold:\n",
    "                self.index2token.append(term)\n",
    "            else:\n",
    "                self.num_pruned += 1\n",
    "                \n",
    "        # now update token2index:\n",
    "        for token in self.index2token[4:]:\n",
    "            self.token2index[token] = self.index2token.index(token)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.index2token)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.token2index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Vocab\n",
    "\n",
    "Let's now test the vocabulary class on the first 10 pairs, showing how it will be used to create sequences of indices from sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '.', 'run', '!', 'run', '!', 'wow', '!', 'fire', '!', 'help', '!', 'jump', '.', 'stop', '!', 'stop', '!', 'stop', '!']\n",
      "\n",
      " ['va', '!', 'cours', '!', 'courez', '!', 'ca', 'alors', '!', 'au', 'feu', '!', 'a', 'l', 'aide', '!', 'saute', '.', 'ca', 'suffit', '!', 'stop', '!', 'arrete', 'toi', '!']\n"
     ]
    }
   ],
   "source": [
    "en_token_sequences = []\n",
    "en_tokens = []\n",
    "fr_token_sequences = []\n",
    "fr_tokens = []\n",
    "for line in raw_pairs[:10]:\n",
    "    pair = tokenise_pair(line)\n",
    "    en_token_sequences.append(pair[0])\n",
    "    en_tokens.extend(pair[0])\n",
    "    fr_token_sequences.append(pair[1])\n",
    "    fr_tokens.extend(pair[1])\n",
    "    \n",
    "print(en_tokens)\n",
    "print('\\n', fr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function Vocab.__init__.<locals>.<lambda> at 0x7f8143692d08>, {'<pad>': 0, '<sos>': 1, '<eos>': 2, '.': 4, 'run': 5, '!': 6, 'stop': 7})\n"
     ]
    }
   ],
   "source": [
    "en_vocab = Vocab(\"en\", en_tokens, prune_at=100)\n",
    "print(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 5], [6, 7], [6, 7], [8, 7], [9, 7], [10, 7], [11, 5], [12, 7], [12, 7], [12, 7]]\n"
     ]
    }
   ],
   "source": [
    "en_index_sequences = []\n",
    "for sentence in en_token_sequences[:10]:\n",
    "    sequence = [en_vocab.token2index[t] for t in sentence]\n",
    "    en_index_sequences.append(sequence)\n",
    "\n",
    "print(en_index_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '.']  :  [4, 5]\n",
      "['run', '!']  :  [6, 7]\n",
      "['run', '!']  :  [6, 7]\n",
      "['wow', '!']  :  [8, 7]\n",
      "['fire', '!']  :  [9, 7]\n",
      "['help', '!']  :  [10, 7]\n",
      "['jump', '.']  :  [11, 5]\n",
      "['stop', '!']  :  [12, 7]\n",
      "['stop', '!']  :  [12, 7]\n",
      "['stop', '!']  :  [12, 7]\n"
     ]
    }
   ],
   "source": [
    "for token_seq, idx_seq in zip(en_token_sequences, en_index_sequences):\n",
    "    print(token_seq, ' : ', idx_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do the same for French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['va', '!']  :  [4, 5]\n",
      "['cours', '!']  :  [6, 5]\n",
      "['courez', '!']  :  [7, 5]\n",
      "['ca', 'alors', '!']  :  [8, 9, 5]\n",
      "['au', 'feu', '!']  :  [10, 11, 5]\n",
      "['a', 'l', 'aide', '!']  :  [12, 13, 14, 5]\n",
      "['saute', '.']  :  [15, 16]\n",
      "['ca', 'suffit', '!']  :  [8, 17, 5]\n",
      "['stop', '!']  :  [18, 5]\n",
      "['arrete', 'toi', '!']  :  [19, 20, 5]\n"
     ]
    }
   ],
   "source": [
    "fr_vocab = Vocab(\"fr\", fr_tokens, prune_at=100)\n",
    "\n",
    "fr_index_sequences = []\n",
    "for sentence in fr_token_sequences[:10]:\n",
    "    sequence = [fr_vocab.token2index[t] for t in sentence]\n",
    "    fr_index_sequences.append(sequence)\n",
    "    \n",
    "for token_seq, idx_seq in zip(fr_token_sequences, fr_index_sequences):\n",
    "    print(token_seq, ' : ', idx_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function Vocab.__init__.<locals>.<lambda> at 0x7f813fe7de18>, {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'va': 4, '!': 5, 'cours': 6, 'courez': 7, 'ca': 8, 'alors': 9, 'au': 10, 'feu': 11, 'a': 12, 'l': 13, 'aide': 14, 'saute': 15, '.': 16, 'suffit': 17, 'stop': 18, 'arrete': 19, 'toi': 20})\n"
     ]
    }
   ],
   "source": [
    "print(fr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Sequences in PyTorch\n",
    "\n",
    "The next step is to convert the sequence of indices into tensors for training and prediction using PyTorch. At it's most basic we can convert an array to a tensor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]  :  tensor([4, 5])\n",
      "[6, 7]  :  tensor([6, 7])\n",
      "[6, 7]  :  tensor([6, 7])\n",
      "[8, 7]  :  tensor([8, 7])\n",
      "[9, 7]  :  tensor([9, 7])\n",
      "[10, 7]  :  tensor([10,  7])\n",
      "[11, 5]  :  tensor([11,  5])\n",
      "[12, 7]  :  tensor([12,  7])\n",
      "[12, 7]  :  tensor([12,  7])\n",
      "[12, 7]  :  tensor([12,  7])\n"
     ]
    }
   ],
   "source": [
    "en_tensor_samples = []\n",
    "for idx_seq in en_index_sequences:\n",
    "    tensor_seq = torch.tensor(idx_seq)\n",
    "    en_tensor_samples.append(tensor_seq)\n",
    "    print(idx_seq, ' : ', tensor_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5]  :  tensor([4, 5])\n",
      "[6, 5]  :  tensor([6, 5])\n",
      "[7, 5]  :  tensor([7, 5])\n",
      "[8, 9, 5]  :  tensor([8, 9, 5])\n",
      "[10, 11, 5]  :  tensor([10, 11,  5])\n",
      "[12, 13, 14, 5]  :  tensor([12, 13, 14,  5])\n",
      "[15, 16]  :  tensor([15, 16])\n",
      "[8, 17, 5]  :  tensor([ 8, 17,  5])\n",
      "[18, 5]  :  tensor([18,  5])\n",
      "[19, 20, 5]  :  tensor([19, 20,  5])\n"
     ]
    }
   ],
   "source": [
    "fr_tensor_samples = []\n",
    "for idx_seq in fr_index_sequences:\n",
    "    tensor_seq = torch.tensor(idx_seq)\n",
    "    fr_tensor_samples.append(tensor_seq)\n",
    "    print(idx_seq, ' : ', tensor_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the basic design pattern above, we now have everything we need to construct a PyTorch dataset. However, before we do so, it will be helpful to look at the generic sequence-to-sequence architecture so that we can get a better understanding of the I/O requirements of the model.\n",
    "\n",
    "## Sequence-to-Sequence Model Architecture\n",
    "\n",
    "A **sequence-to-sequence** architecture is a generic type of deep learning architecture consisting of two main components:\n",
    "\n",
    "- An **encoder** which takes in an **input sequence** and encodes it into some **latent representation**, sometimes called the **context vector**.\n",
    "- A **decoder** which takes in the latent representation and produces an **output sequence**.\n",
    "    - The decoder may also take in an input sequence of its own. During training this is typically the target sequence. During prediction, this is the predicted sequence itself.\n",
    "    - During training the decoder can be thought of as a **generative model** which tries to produce the target sequence, given the context vector.\n",
    "\n",
    "For example, in the French $\\rightarrow$ English translation task, a sequence-to-sequence model will look as follows\n",
    "\n",
    "<img src=\"assets/seq2seq.png\">\n",
    "\n",
    "Here the output sequence is fed back into the decoder for prediction. \n",
    "\n",
    "One thing to note is that for the encoder we can process the entire sequence at once since we are only interested in the final hidden state of the encoder, however since the decoder is a generative model it does not make sense to process the entire sequence as a batch since the decoder at time $t$ will depend on its output at time $t-1$. Thus the decoder will typically be implemented with a loop in the forward pass.\n",
    "\n",
    "## Sequence Batching\n",
    "\n",
    "Typically we would like to process the data in minibatches. At first this might seem confusing since the decoder needs to loop through each sequence one element at a time, however the PyTorch implementations of recurrent modules support batch processing provided that its input has shape `sequence_length x batch_size x input_dimension` where _input dimension_ will be the _embedding dimension_. This way we can loop over each sequence in the batch one element at a time by indexing over the first dimension. \n",
    "\n",
    ">Since we're using embeddings, the dimensionality of the batch will be transformed as:\n",
    "<br/><br/>`sequence_length x batch_size` $\\rightarrow$ embedding $\\rightarrow$ `sequence_length x batch_size x embedding_dimension` $\\rightarrow$ recurrent unit\n",
    "\n",
    "We'll see later that _sequence length_ will have to be the _max sequence length_ of the batch.\n",
    "\n",
    "Let's see how this works by considering the test tensors we created earlier for the English samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 6,  7],\n",
      "        [ 8,  7],\n",
      "        [ 9,  7],\n",
      "        [10,  7],\n",
      "        [11,  5],\n",
      "        [12,  7],\n",
      "        [12,  7],\n",
      "        [12,  7]])\n",
      "\n",
      " torch.Size([10, 2])\n",
      "\n",
      " sequence at batch index 0:  tensor([4, 5])\n"
     ]
    }
   ],
   "source": [
    "en_tensor_batch = torch.stack(en_tensor_samples)\n",
    "\n",
    "print(en_tensor_batch)\n",
    "print('\\n', en_tensor_batch.size())\n",
    "print('\\n', 'sequence at batch index 0: ', en_tensor_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we will need to reshape this batch so that indexing over the first dimension corresponds to time (indexing each sequence at each time step) and not to sample (indexing over each batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  6,  6,  8,  9, 10, 11, 12, 12, 12],\n",
      "        [ 5,  7,  7,  7,  7,  7,  5,  7,  7,  7]])\n",
      "\n",
      " torch.Size([2, 10])\n",
      "\n",
      " sequence elements at time 0:  tensor([ 4,  6,  6,  8,  9, 10, 11, 12, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "en_tensor_batch = en_tensor_batch.transpose(1, 0)\n",
    "\n",
    "print(en_tensor_batch)\n",
    "print('\\n', en_tensor_batch.size())\n",
    "print('\\n', 'sequence elements at time 0: ', en_tensor_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now indexing in the first dimension (by $t$) gives us a subset of the batch where the $i^{th}$ element corresponds to the $t^{th}$ timestep of the $i^{th}$ sequence in the batch. This is the dimensionality which we will require from batching during the dataloading process.\n",
    "\n",
    "### Batch Sequence Padding\n",
    "\n",
    "You may have noticed that all the tensors in the English samples conveniently have the same length whereas with the French samples the tensors have different lengths. This will be problematic for indexing over time as we will either get index-out-of-bounds errors for shorter sequences or we will fail to index longer sequences fully. \n",
    "\n",
    "The way we deal with this is sequence padding to the right to the length of the longest sequence in the batch. Earlier you may have noticed that the first index $0$ in each vocabulary was reserved for padding. By padding with $0$ no information will be lost or gained when transforming padded sequences through each recurrent unit.\n",
    "\n",
    ">**Note:** It's worth mentioning that in cases where there are very long sequences in the training data, then a maximum sequence length parameter is usually set to control memory during processing. Sequences exceding the length limit can then be rolled over onto the next batch.\n",
    "For example if the sequence length limit is $4$ but we have a sequence, `[4, 5, 9, 12, 31]` of length $5$ then it will be rolled over the batch as \n",
    "```python\n",
    "tensor([[ 4,  5,  9, 12],\n",
    "        [31,  0,  0,  0]])\n",
    "```\n",
    "which is transposed to\n",
    "```python\n",
    "tensor([[ 4, 31],\n",
    "        [ 5,  0],\n",
    "        [ 9,  0],\n",
    "        [12,  0]]),\n",
    "```\n",
    "\n",
    "\n",
    "Let's see how this padding will work in practice using the `nn.utils.rnn.pad_sequence` utility. First notice what happens when we don't pad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2 and 3 in dimension 1 at /opt/conda/conda-bld/pytorch_1549635019666/work/aten/src/TH/generic/THTensorMoreMath.cpp:1307",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-6f130662bd50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfr_tensor_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tensor_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tensor_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2 and 3 in dimension 1 at /opt/conda/conda-bld/pytorch_1549635019666/work/aten/src/TH/generic/THTensorMoreMath.cpp:1307"
     ]
    }
   ],
   "source": [
    "fr_tensor_batch = torch.stack(fr_tensor_samples)\n",
    "\n",
    "print(fr_tensor_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  6,  7,  8, 10, 12, 15,  8, 18, 19],\n",
      "        [ 5,  5,  5,  9, 11, 13, 16, 17,  5, 20],\n",
      "        [ 0,  0,  0,  5,  5, 14,  0,  5,  0,  5],\n",
      "        [ 0,  0,  0,  0,  0,  5,  0,  0,  0,  0]]) \n",
      "\n",
      "sequence elements at time 0: tensor([ 4,  6,  7,  8, 10, 12, 15,  8, 18, 19])\n",
      "sequence elements at time 1: tensor([ 5,  5,  5,  9, 11, 13, 16, 17,  5, 20])\n",
      "sequence elements at time 2: tensor([ 0,  0,  0,  5,  5, 14,  0,  5,  0,  5])\n",
      "sequence elements at time 3: tensor([0, 0, 0, 0, 0, 5, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "fr_tensors_padded = nn.utils.rnn.pad_sequence(fr_tensor_samples)\n",
    "\n",
    "print(fr_tensors_padded, '\\n')\n",
    "\n",
    "for t in range(4):\n",
    "    print(f'sequence elements at time {t}: {fr_tensors_padded[t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently PyTorch's RNN padding utility also transposes the sequences into the right dimensionality for PyTorch's reccurent modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Embeddings\n",
    "\n",
    "Before we pass the batched sequences through the recurrent cell, we need to pass them through an embedding layer. In PyTorch we can instruct the embedding layer to preserve the padding index, so that sequence length information is not lost during the embedding transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(21, 3, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "fr_embedding = nn.Embedding(num_embeddings=len(fr_vocab), embedding_dim=3, padding_idx=0)\n",
    "\n",
    "print(fr_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite interesting to see what the embedding look like. The dimensionality for sequence data is quite confusing but as we will see, by extracting timesteps from the embedded batch, we can try and understand its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3826,  0.2643, -0.4163],\n",
      "         [-1.1180,  0.2400, -0.6511],\n",
      "         [-0.1632, -0.2048,  0.2239],\n",
      "         [-0.0987,  1.3309,  2.1294],\n",
      "         [-0.0869, -1.1208,  0.2758],\n",
      "         [ 0.3475,  2.1952, -0.4877],\n",
      "         [ 0.4658, -0.7564,  0.6001],\n",
      "         [-0.0987,  1.3309,  2.1294],\n",
      "         [-0.9418, -0.6638,  0.9538],\n",
      "         [ 0.2485,  0.1805, -1.0135]],\n",
      "\n",
      "        [[ 0.9746,  0.4226, -0.5363],\n",
      "         [ 0.9746,  0.4226, -0.5363],\n",
      "         [ 0.9746,  0.4226, -0.5363],\n",
      "         [-0.9805,  0.3202,  1.0725],\n",
      "         [-0.2719, -0.2972,  0.6684],\n",
      "         [-1.2755,  0.6203, -0.8780],\n",
      "         [ 0.2191,  1.0461,  0.8399],\n",
      "         [-0.5260,  0.9915,  0.1009],\n",
      "         [ 0.9746,  0.4226, -0.5363],\n",
      "         [ 1.0638,  0.3519, -1.0559]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9746,  0.4226, -0.5363],\n",
      "         [ 0.9746,  0.4226, -0.5363],\n",
      "         [-0.1795,  0.4190,  1.5105],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9746,  0.4226, -0.5363],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9746,  0.4226, -0.5363]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9746,  0.4226, -0.5363],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "fr_embedded_tensors = fr_embedding(fr_tensors_padded)\n",
    "\n",
    "print(fr_embedded_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret how the embedding has transformed the padded batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before embedding:  torch.Size([4, 10])\n",
      "After embedding:  torch.Size([4, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "print('Before embedding: ', fr_tensors_padded.size())\n",
    "print('After embedding: ', fr_embedded_tensors.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sequence was previously represented columnwise in the batch. Now each sequence is a 2-dimensional slice along the first and third dimension of the batch (which we can think of as a 3d volume of \"slices\" where each slice is a sequence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before embedding:\n",
      "\n",
      "tensor([4, 5, 0, 0])\n",
      "\n",
      "After embedding:\n",
      "\n",
      "tensor([[ 0.3826,  0.2643, -0.4163],\n",
      "        [ 0.9746,  0.4226, -0.5363],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Before embedding:\\n')\n",
    "print(fr_tensors_padded[:, 0])\n",
    "print('\\nAfter embedding:\\n')\n",
    "print(fr_embedded_tensors[:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the padding has been preserved. \n",
    "\n",
    "Also notice that **time** is still indexed along the first dimension of the tensor (going downwards). This will still hold true for the entire batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3826,  0.2643, -0.4163],\n",
      "        [-1.1180,  0.2400, -0.6511],\n",
      "        [-0.1632, -0.2048,  0.2239],\n",
      "        [-0.0987,  1.3309,  2.1294],\n",
      "        [-0.0869, -1.1208,  0.2758],\n",
      "        [ 0.3475,  2.1952, -0.4877],\n",
      "        [ 0.4658, -0.7564,  0.6001],\n",
      "        [-0.0987,  1.3309,  2.1294],\n",
      "        [-0.9418, -0.6638,  0.9538],\n",
      "        [ 0.2485,  0.1805, -1.0135]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(fr_embedded_tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each row of this slice corresponds to the _first time step_ of each sequence. See how the 3-dimensional tensor embedding for index $4$, <br/>`[0.3826,  0.2643, -0.4163]`, matches the first time step of the first sequence.\n",
    "\n",
    "### Batched Forward Propagation\n",
    "\n",
    "The embedded batch now has the right dimensionality for processing through a recurrent cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "print(fr_embedded_tensors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=3,  # embedding_dimension = 3\n",
    "             hidden_size=4,\n",
    "             num_layers=1,\n",
    "             bidirectional=False) # num_directions = 1\n",
    "\n",
    "h_0 = torch.randn(1, 10, 4)  # num_layers*num_directions x batch_size x hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "output, h_n = rnn(fr_embedded_tensors, h_0)  # output corresponds to final time step: t = n = max_seq_len = 4\n",
    "\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted we could also process the batch sequentially through time as we will in the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 4])\n",
      "torch.Size([1, 10, 4])\n",
      "torch.Size([1, 10, 4])\n",
      "torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for t in range(4):\n",
    "    embeddings_at_time_t = fr_embedded_tensors[t].view(1, 10, 3)  # 1 x batch_size x embedding_dimension\n",
    "    output_t, h_t = rnn(embeddings_at_time_t, h_0)\n",
    "    outputs.append(output_t)\n",
    "    \n",
    "    print(output_t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Sequence Dataset\n",
    "\n",
    "Now that we have an understanding of the correct tensor representation of the dataset, we can now construct a PyTorch `Dataset` class which will convert the data into tensors with the correct shape. We will then pass this dataset to a PyTorch `Dataloader` which will handle batching. It is through the dataloader class that the model will interface with the dataset.\n",
    "\n",
    "When interfacing with sequence models we will want to focus as much time on the modelling choices so having gone through this whole notebook to implement a preprocessing strategy we'll ask this dataset class to not just handle tensor conversion but the whole data preprocessing from file to tensor representation. \n",
    "\n",
    "To recap this means that the dataset class will:\n",
    "\n",
    "- read sentence pairs from a file\n",
    "- normalise each sentence in each pair\n",
    "- tokenise each pair\n",
    "- build a vocabulary from the tokenised pairs for each language\n",
    "- convert every pair into sequences of indices for each language\n",
    "- convert every pair into tensors\n",
    "\n",
    "During training and evaluation all we will ask of the dataloader class is to simply\n",
    "\n",
    "- batch pairs of tensor sequences\n",
    "- order input and target pairs by input length within each batch\n",
    "- pad the input and target pairs respectively\n",
    "\n",
    "Note that in the dataloader class padding will be applied locally to each batch using a special collate function. This is because the longest sequence length can differ from batch to batch, although it will be limited by `max_seq_length` throughout.\n",
    "\n",
    "> **Note:** For this implementation we will use a special filter to limit the size of the dataset to reduce complexity. This will make it easier and faster to test implementations on limit resources. It will also mean that we won't encounter the sequence rollover problem when sequence length exceeds the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, lang1_name, lang2_name, max_seq_length, \n",
    "                 prune_at, filter_pairs=False, reverse=False):\n",
    "        \"\"\"PyTorch dataset for translation data stored as tab separated sentence pairs, with each\n",
    "        pair on one line of the file. The input language - lang1 - is the first sentence and the\n",
    "        output language - lang2 - is the second sentence by default.\n",
    "            \n",
    "                  line:  'sentence1_lang1 \\t sentence1_lang2'\n",
    "        \n",
    "            path (str): Path to text file containing tab separated sentence pairs on each line.                    \n",
    "            lang1_name (str): Name to use for lang1 vocabulary.\n",
    "            lang2_name (str): Name to use for lang2 vocabulary.\n",
    "            max_seq_length (int): Maximum length of each sequence. If sequence \n",
    "                exceeds the max length it will be split into multiple \n",
    "                sequences.\n",
    "            prune_at (int): \n",
    "            filter_pairs (bool): If true will apply a special filter to simplify the\n",
    "                dataset for faster experimentation.\n",
    "            reverse (bool): If true, will treat lang2 as the input and lang1 as the target\n",
    "        \n",
    "        TODO: implement sequence rollover for long sequences.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.reverse = reverse\n",
    "        \n",
    "        raw_pairs = self._read_pairs(path, filter_pairs)\n",
    "\n",
    "        self.tokenised_pairs = [self._tokenise_pair(pair) for pair in raw_pairs]\n",
    "        if self.reverse:\n",
    "            self.tokenised_pairs = [list(reversed(pair)) for pair in self.tokenised_pairs]\n",
    "        self.input_vocab, self.output_vocab = self._init_vocabs(lang1_name, lang2_name, prune_at)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        pair = self.tokenised_pairs[index]\n",
    "        \n",
    "        input_sequence = self.tokens2sequence(pair[0], self.input_vocab)\n",
    "        target_sequence = self.tokens2sequence(pair[1], self.output_vocab, append_EOS=True)\n",
    "        \n",
    "        return input_sequence, target_sequence\n",
    "        \n",
    "    def tokens2sequence(self, tokens, vocab, append_EOS=False):\n",
    "        sequence = [vocab.token2index[t] for t in tokens]\n",
    "        if append_EOS:\n",
    "            sequence.append(EOS_TOKEN)\n",
    "        return torch.tensor(sequence)\n",
    "    \n",
    "    def _read_pairs(self, path, filter_pairs):\n",
    "        raw_pairs = open(data_path, encoding='utf-8').read().strip().split('\\n')\n",
    "        print(f'Total number of sentence pairs: {len(raw_pairs)}')\n",
    "        \n",
    "        if filter_pairs:\n",
    "            raw_pairs = [pair for pair in raw_pairs if self._keep(pair)]\n",
    "            print(f'Total number of pairs after filtering: {len(raw_pairs)}')\n",
    "            \n",
    "        return raw_pairs\n",
    "        \n",
    "    def _init_vocabs(self, lang1_name, lang2_name, prune_at):\n",
    "        \n",
    "        if self.reverse:\n",
    "            input_name = lang2_name\n",
    "            output_name = lang1_name\n",
    "        else:\n",
    "            input_name = lang1_name\n",
    "            output_name = lang2_name\n",
    "\n",
    "        input_tokens = []\n",
    "        output_tokens = []\n",
    "        for pair in self.tokenised_pairs:\n",
    "            \n",
    "            input_tokens.extend(pair[0])\n",
    "            output_tokens.extend(pair[1])\n",
    "            \n",
    "        input_vocab = Vocab(input_name, input_tokens, prune_at)\n",
    "        output_vocab = Vocab(output_name, output_tokens, prune_at)\n",
    "                \n",
    "        print('Total number of tokens in each vocabulary:')\n",
    "        print(f'\\tInput: {input_vocab.name}: {len(input_vocab)} (num pruned: {input_vocab.num_pruned})')\n",
    "        print(f'\\tOutput: {output_vocab.name}: {len(output_vocab)} (num pruned: {output_vocab.num_pruned})')\n",
    "            \n",
    "        return input_vocab, output_vocab\n",
    "    \n",
    "    def _tokenise_pair(self, pair):\n",
    "        return [self._tokenise(self._normalise_string(s)) for s in pair.split('\\t')]\n",
    "    \n",
    "    def _tokenise(self, sentence):\n",
    "        #TODO: add sequence rollover here\n",
    "        return sentence.split(' ')\n",
    "    \n",
    "    def _normalise_string(self, s):\n",
    "        s = self._unicode_normalisation(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)  # add space in front of \".\", \"!\" \"?\"\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # remove unwanted characets\n",
    "        return s\n",
    "        \n",
    "    def _unicode_normalisation(self, s):\n",
    "        # ref: https://stackoverflow.com/a/518232/2809427\n",
    "        return ''.join(\n",
    "            char for char in unicodedata.normalize('NFD', s)  # \"À\" -->  \"A\" + \"̖\"\n",
    "            if unicodedata.category(char) != 'Mn')  # remove \"̖\"s\n",
    "\n",
    "    def _keep(self, pair):\n",
    "        \"\"\"Special filter for reducing the size and complexity of the dataset. \n",
    "\n",
    "        Retains only pairs where the English sentence is of the form \n",
    "\n",
    "            \"He is ... \", \"She is ... \" etc\n",
    "\n",
    "        And filters pairs which are longer than max_seq_length. Note that this\n",
    "        means that we will not encounter sequence rollover issues when batching.\n",
    "\n",
    "        \"\"\"\n",
    "        en_sent_prefixes = (\"I am\", \"I'm\",\n",
    "                             \"He is\", \"He's\",\n",
    "                             \"She is\", \"She's\",\n",
    "                             \"You are\", \"You're\",\n",
    "                             \"We are\", \"We're \",\n",
    "                             \"They are\", \"They're\")\n",
    "\n",
    "        split_pair = pair.split('\\t')\n",
    "\n",
    "        en_sentence = split_pair[0]\n",
    "        fr_sentence = split_pair[1]\n",
    "\n",
    "        condition =  len(fr_sentence.split(' ')) < self.max_seq_length and \\\n",
    "                     len(en_sentence.split(' ')) < self.max_seq_length and en_sentence.startswith(en_sent_prefixes)\n",
    "\n",
    "        return condition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenised_pairs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentence pairs: 135842\n",
      "Total number of pairs after filtering: 11893\n",
      "Total number of tokens in each vocabulary:\n",
      "\tInput: en: 3058 (num pruned: 0)\n",
      "\tOutput: fr: 4715 (num pruned: 0)\n"
     ]
    }
   ],
   "source": [
    "en_fr_dataset = TranslationDataset(data_path, 'en', 'fr', \n",
    "                                   max_seq_length=10, \n",
    "                                   prune_at=100,\n",
    "                                   filter_pairs=True,\n",
    "                                   reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to translate from French $\\rightarrow$ English instead, then the `reverse` toggle will reverse the order of the tab separated pairs so that the French sentences are first and the English sentences second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentence pairs: 135842\n",
      "Total number of pairs after filtering: 11893\n",
      "Total number of tokens in each vocabulary:\n",
      "\tInput: fr: 4715 (num pruned: 0)\n",
      "\tOutput: en: 3058 (num pruned: 0)\n"
     ]
    }
   ],
   "source": [
    "rev_en_fr_dataset = TranslationDataset(data_path, 'en', 'fr', \n",
    "                                       max_seq_length=10, \n",
    "                                       prune_at=100,\n",
    "                                       filter_pairs=True, \n",
    "                                       reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's Dataloader class only needs to be able to index the dataset to collate batches. We can see how this works below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 5, 6]), tensor([4, 5, 6, 7, 2]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_dataset[0]  # recall EOS_TOKEN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  5, 14,  6]), tensor([ 8, 13, 23,  7,  2]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index the reversed dataset to see how the input and output pairs have been swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 5, 6, 7]), tensor([4, 5, 6, 2]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_en_fr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8, 13, 23,  7]), tensor([ 4,  5, 14,  6,  2]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_en_fr_dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the EOS token is correctly append to only the output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataloader Batching\n",
    "\n",
    "Finally let's see how we will interface with the dataset during training and evaluation using a PyTorch `Datalaoder`. Because PyTorch's Dataloader default collate function stacks tensors to form a batch, we will be unable to use the default collate function for sequences of variable lengths. So we will write our own collate function to unzip a list of tuples of input, target sequences, pad them seperately and then rezip the tuple as the padded batch.\n",
    "\n",
    "PyTorch recurrent modules support a special type of sequence batch called a `PackedSequence` which can be used to calculate gradients on variable length sequences _while ignoring the padded values_. Unfortunately the embedding module does not support packed sequences so we will still have to pad each batch for the embedding, then use the `nn.utils.rnn.pack_padded_sequence` utility to pack the output of the embedding layer, which as we saw can be set to preserve padding.\n",
    "\n",
    "To use the `pack_padded_sequence` utility we will have to sort the input tensors by their length and pass a tensor of lengths to pack the batch correctly. To maintain correspondence between the input and target sequences, we will sort both by the lengths of the input sequences. \n",
    "\n",
    "> **Note:** Since we don't need to pack the target sequences - as the decoder is an autoregressive model which relies on its output at the previous time step and is thus computed one time step at a time -  we will not need to sort the target sequences by their lengths (which would break correspondence in the batch).\n",
    "\n",
    "See this [notebook](sequence-tensor-batching.ipynb) for an exploration of maintaining correspondence for packed sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_collate(batch: list):\n",
    "    # we need to sort the input batch in descending order by length\n",
    "    # in order to use PyTorch PackedSequences. We presort the batch\n",
    "    # so that input and target sequence correspondence is mainatined\n",
    "    presorted_batch = sorted(batch, key=lambda b: len(b[0]), reverse=True)\n",
    "    \n",
    "    input_presorted, target_presorted = zip(*presorted_batch)\n",
    "    input_lengths = torch.tensor([len(seq) for seq in input_presorted])\n",
    "\n",
    "    padded_input = nn.utils.rnn.pad_sequence(input_presorted)\n",
    "    padded_target = nn.utils.rnn.pad_sequence(target_presorted)\n",
    "    \n",
    "    return padded_input, padded_target, input_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the collate function works, let's construct a test batch of two sequence pairs and examine. Notice that each pair is a tuple so that at its most basic, the batch is a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([4, 5, 6]), tensor([4, 5, 6, 7, 2])),\n",
       " (tensor([4, 5, 7, 6]), tensor([ 8,  9, 10,  7,  2]))]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = [en_fr_dataset[0], en_fr_dataset[1]]\n",
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_batch, test_target_batch, input_lengths = pad_and_collate(test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how the input sequence of each pair (tuple) has been extracted and padded independently of the target sequence of each pair. Notice how the input batch has been sorted by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4],\n",
       "        [5, 5],\n",
       "        [7, 6],\n",
       "        [6, 0]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the target batch from our test batch. Notice how target batch has been sorted such that the input and target pairs are perfectly aligned along the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8,  4],\n",
       "        [ 9,  5],\n",
       "        [10,  6],\n",
       "        [ 7,  7],\n",
       "        [ 2,  2]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_dataloader = DataLoader(en_fr_dataset, \n",
    "                              batch_size=10, \n",
    "                              num_workers=0, \n",
    "                              collate_fn=pad_and_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_data_iterator = iter(en_fr_dataloader)\n",
    "first_batch = next(en_fr_data_iterator)\n",
    "\n",
    "first_input_batch, first_target_batch, first_input_lengths = first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "tensor([[ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
      "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5],\n",
      "        [ 7,  7,  8,  8,  9, 10, 10, 12, 13,  6],\n",
      "        [ 6,  6,  6,  6,  6, 11, 11,  6,  6,  0]])\n",
      "\n",
      "Target batch:\n",
      "tensor([[ 8, 11,  8,  8,  8,  8,  8,  8,  8,  4],\n",
      "        [ 9, 12, 13, 13, 13, 13, 13, 13, 13,  5],\n",
      "        [10,  7, 14, 15, 16, 18, 20, 21, 22,  6],\n",
      "        [ 7,  2,  7,  7, 17, 19, 19,  7,  7,  7],\n",
      "        [ 2,  0,  2,  2,  7,  2,  2,  2,  2,  2],\n",
      "        [ 0,  0,  0,  0,  2,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "print('Input batch:')\n",
    "print(first_input_batch)\n",
    "print('\\nTarget batch:')\n",
    "print(first_target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We now have a dataloader class which can pad input and target batches and sort the input batch while ensuring that the target batch is aligned to the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Sequence Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embedding = nn.Embedding(num_embeddings=len(en_fr_dataset.input_vocab), embedding_dim=3, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_input = en_embedding(first_input_batch)\n",
    "print(embedded_input.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (axiom)\n",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
