{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Gradients\n",
    "\n",
    "PyTorch tensors have an attribute `requires_grad` which tells PyTorch (and Autograd) to track all operations on it. To compute the gradients of these operations, use `.backward()`. The gradients of the tensor will then be accumulated into the `.grad` attribute.\n",
    "\n",
    "To stop a tensor from tracking its gradients, use `.detach()`. Another option is to temporarily wrap evaluation calls in a `with` code block using `torch.no_grad():`. This is useful when evaluating a model which has trainable parameters, but we don't require the gradients at evaluation time.\n",
    "\n",
    "### PyTorch Functions and Gradients\n",
    "\n",
    "The PyTorch `Function` class is used to track gradient operations on tensors. Under the hood, PyTorch builds an acyclic computation graph with gradients tracked along the edges and accumulated in each output tensor along the graph. To help keep track of function gradients, each tensor has a `.grad_fn` attribute which references the function which created the tensor. \n",
    "\n",
    "> **Note:** User-created tensors have `.grad_fn is None`.\n",
    "\n",
    "To compute derivates call `.backward()`. If the tensor is non-sclar, then a `gradient` arguments needs to be specified as a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a tensor and track computations on it\n",
    "x = torch.ones(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# do a tensor operation\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fb464947f28>\n"
     ]
    }
   ],
   "source": [
    "# y was created as the result of an operation (function) and that has a `.grad_fn`\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# do more operations\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7fb46495f320>\n"
     ]
    }
   ],
   "source": [
    "# Note that `.requires_grad_()` is an in-place tensor operation\n",
    "a = torch.randn(2,2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlier we created a tensor called out - let's backprop its gradients\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-Jacobian Products\n",
    "\n",
    "`torch.autograd` doesn't calculate symbolic gradients. Rather it calculates the gradient evaluated at a point. \n",
    "\n",
    "If $\\vec{y} = f(\\vec{x})$, then the gradient of $\\vec{y}$ w.r.t $\\vec{x}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "J = \\begin{bmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n}\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Then for any vector $\\vec{v} = (v_1, \\cdots, v_m)^T$, `torch.autograds` calculates the **vector-Jacobian** product:\n",
    "\n",
    "\\begin{align}\n",
    "dv = v^T \\cdot J\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 2.0000, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "# for a concreate example of this operation:\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2  # f(x)  = x^2\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense since if $y = f(x) = x^2$, then\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla f = \\begin{bmatrix} 2 & 2 &2 \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "therefore:\n",
    "\\begin{align}\n",
    "v^T\\cdot\\nabla f = & \\ \\begin{bmatrix} 0.1 \\\\ 1.0 \\\\ 0.001 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 & 2 &2 \\end{bmatrix} \\\\\n",
    "                   & \\\\\n",
    "                   & = \\begin{bmatrix} 0.2 & 2.0 & 0.002 \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (axiom)\n",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
