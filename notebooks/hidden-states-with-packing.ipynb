{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Hidden State Length Correspondence\n",
    "\n",
    "**|| Jonty Sinai ||** 02-05-2019\n",
    "\n",
    "One thing we want to do is ensure that hidden states have been propagated correctly for each sequence in the batch. To see why, consider the following (sorted) batch of input sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1047b2e70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "random.seed(1901)\n",
    "np.random.seed(1901)\n",
    "torch.manual_seed(1901)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 0, 0],\n",
      "        [1, 2, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [torch.tensor([1, 1, 1, 1, 1]),\n",
    "               torch.tensor([2, 2, 2, 2]),\n",
    "               torch.tensor([3, 3, 3]),\n",
    "               torch.tensor([4, 4]),\n",
    "               torch.tensor([5, 5])]\n",
    "\n",
    "test_lengths = torch.tensor([len(seq) for seq in test_inputs], dtype=torch.long)\n",
    "\n",
    "test_inputs_padded = rnn_utils.pad_sequence(test_inputs)\n",
    "print(test_inputs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2594,  0.6924],\n",
      "         [-1.2594,  0.6924],\n",
      "         [-1.2594,  0.6924],\n",
      "         [-1.2594,  0.6924],\n",
      "         [-1.2594,  0.6924]]])\n"
     ]
    }
   ],
   "source": [
    "test_embedding = nn.Embedding(num_embeddings=6, embedding_dim=3, padding_idx=0)\n",
    "\n",
    "test_rnn = nn.RNN(input_size=3, hidden_size=2)\n",
    "\n",
    "test_h0 = torch.randn(1, 1, 2).repeat(1, 5, 1)\n",
    "\n",
    "print(test_h0)  # same h_0 for every index in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now compare final hidden states when we pack sequences and compute on the entire batch, then when we compute on each sequence, one at a time.\n",
    "\n",
    "### Batched Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2400, -0.6511, -0.1632],\n",
      "         [-0.2048,  0.2239,  0.7058],\n",
      "         [-0.4830,  2.1294, -0.9805],\n",
      "         [ 0.3202,  1.0725, -0.0869],\n",
      "         [-1.1208,  2.4741,  0.7153]],\n",
      "\n",
      "        [[ 0.2400, -0.6511, -0.1632],\n",
      "         [-0.2048,  0.2239,  0.7058],\n",
      "         [-0.4830,  2.1294, -0.9805],\n",
      "         [ 0.3202,  1.0725, -0.0869],\n",
      "         [-1.1208,  2.4741,  0.7153]],\n",
      "\n",
      "        [[ 0.2400, -0.6511, -0.1632],\n",
      "         [-0.2048,  0.2239,  0.7058],\n",
      "         [-0.4830,  2.1294, -0.9805],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2400, -0.6511, -0.1632],\n",
      "         [-0.2048,  0.2239,  0.7058],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2400, -0.6511, -0.1632],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_embedded = test_embedding(test_inputs_padded)\n",
    "\n",
    "print(test_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0494, -0.3800],\n",
      "         [-0.1983, -0.3963],\n",
      "         [-0.6183, -0.8498],\n",
      "         [-0.7652, -0.2736],\n",
      "         [-0.5619, -0.8457]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_packed_input = rnn_utils.pack_padded_sequence(test_embedded, test_lengths)\n",
    "\n",
    "test_output_packed, test_hn_batch = test_rnn(test_packed_input, test_h0)\n",
    "\n",
    "print(test_hn_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled-based computation\n",
    "\n",
    "Now we would like to confirm that we get the same hidden state by indexing into the $j^{th}$ sequence (2nd index) in each batch. We have the lengths of each sequence, so we can ignore the padding index by only computing up to these lengths.\n",
    "\n",
    "Let's see how we can grab one sequence from the embeddings which have dimension `max_seq_length x batch_size x embedding_dim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4830,  2.1294, -0.9805],\n",
       "        [-0.4830,  2.1294, -0.9805],\n",
       "        [-0.4830,  2.1294, -0.9805],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedded[:, 2, :]  # all time steps from the third sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, all values are the same and it has length 3 corresponding to the seqeunce `[3, 3, 3]`. Let's ensure that it has the right shape for the RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(test_embedded[:, 0, :].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to view this tensor as a `seq_len x 1 x embedding_dim` sequence (i.e. batch size of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4830,  2.1294, -0.9805]],\n",
       "\n",
       "        [[-0.4830,  2.1294, -0.9805]],\n",
       "\n",
       "        [[-0.4830,  2.1294, -0.9805]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedded[:, 2, :].view(5, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the hidden state is initiliased as a `1 x batch_size x hidden_state` tensor with the same hidden state across the batch dimension. Let's grab this hidden state which we will use for computation for one sequence at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2594,  0.6924]]])\n"
     ]
    }
   ],
   "source": [
    "test_h0_single = test_h0[0, 0, :].view(1, 1, 2)\n",
    "\n",
    "print(test_h0_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = []\n",
    "test_hn_states = []\n",
    "\n",
    "for j in range(5):\n",
    "    seq_length = test_lengths[j]\n",
    "    seq_embedded = test_embedded[:, j, :].view(5, 1, 3)[:seq_length]\n",
    "    \n",
    "    out_j, h_t_j = test_rnn(seq_embedded, test_h0_single)\n",
    "    \n",
    "    test_outputs.append(out_j)\n",
    "    test_hn_states.append(h_t_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbatched:\n",
      "tensor([[[-0.0494, -0.3800]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.1983, -0.3963]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.6183, -0.8498]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.7652, -0.2736]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.5619, -0.8457]]], grad_fn=<StackBackward>)\n",
      "\n",
      "Batched:\n",
      "tensor([[[-0.0494, -0.3800],\n",
      "         [-0.1983, -0.3963],\n",
      "         [-0.6183, -0.8498],\n",
      "         [-0.7652, -0.2736],\n",
      "         [-0.5619, -0.8457]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Unbatched:')\n",
    "for j in range(5):\n",
    "    print(test_hn_states[j])\n",
    "\n",
    "print('\\nBatched: Matches')\n",
    "print(test_hn_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the hidden states are correctly calculated only up to the right sequence length for each sequence using packed sequences.\n",
    "\n",
    "Similary for outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      "\n",
      "Unbatched:\n",
      "tensor([[ 0.5373, -0.7590],\n",
      "        [-0.5505,  0.0691],\n",
      "        [ 0.1044, -0.4968],\n",
      "        [-0.3366, -0.1353],\n",
      "        [-0.0494, -0.3800]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Batched:\n",
      "tensor([[ 0.5373, -0.7590],\n",
      "        [-0.5505,  0.0691],\n",
      "        [ 0.1044, -0.4968],\n",
      "        [-0.3366, -0.1353],\n",
      "        [-0.0494, -0.3800]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 1:\n",
      "\n",
      "Unbatched:\n",
      "tensor([[ 0.6832, -0.8781],\n",
      "        [-0.4492, -0.2140],\n",
      "        [ 0.2100, -0.6302],\n",
      "        [-0.1983, -0.3963]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Batched:\n",
      "tensor([[ 0.6832, -0.8781],\n",
      "        [-0.4492, -0.2140],\n",
      "        [ 0.2100, -0.6302],\n",
      "        [-0.1983, -0.3963],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 2:\n",
      "\n",
      "Unbatched:\n",
      "tensor([[-0.0927, -0.9743],\n",
      "        [-0.7976, -0.7854],\n",
      "        [-0.6183, -0.8498]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Batched:\n",
      "tensor([[-0.0927, -0.9743],\n",
      "        [-0.7976, -0.7854],\n",
      "        [-0.6183, -0.8498],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 3:\n",
      "\n",
      "Unbatched:\n",
      "tensor([[ 0.0049, -0.8728],\n",
      "        [-0.7652, -0.2736]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Batched:\n",
      "tensor([[ 0.0049, -0.8728],\n",
      "        [-0.7652, -0.2736],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 4:\n",
      "\n",
      "Unbatched:\n",
      "tensor([[ 0.5847, -0.9850],\n",
      "        [-0.5619, -0.8457]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Batched:\n",
      "tensor([[ 0.5847, -0.9850],\n",
      "        [-0.5619, -0.8457],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_output_unpacked, _ = rnn_utils.pad_packed_sequence(test_output_packed)\n",
    "\n",
    "for j in range(5):\n",
    "    print(f'\\nSample {j}:\\n')\n",
    "    print('Unbatched:')\n",
    "    print(test_outputs[j][:, 0, :])\n",
    "\n",
    "    print('\\nBatched: Matches')\n",
    "    print(test_output_unpacked[:, j, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the output have been correctly computed with padding ignored "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage In, Garbage Out\n",
    "\n",
    "Finally let's see what happens when we don't pay attention to sequenc length and just pass a padded tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_padded, hn_padded = test_rnn(test_embedded, test_h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed:\n",
      "tensor([[[-0.0494, -0.3800]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.1983, -0.3963]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.6183, -0.8498]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.7652, -0.2736]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.5619, -0.8457]]], grad_fn=<StackBackward>)\n",
      "\n",
      "Unpacked: Mismatch\n",
      "tensor([[[-0.0494, -0.3800],\n",
      "         [-0.2015, -0.4695],\n",
      "         [-0.1950, -0.5119],\n",
      "         [-0.1184, -0.5026],\n",
      "         [-0.2403, -0.4121]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print('Packed:')\n",
    "for j in range(5):\n",
    "    print(test_hn_states[j])\n",
    "\n",
    "print('\\nUnpacked: Mismatch')\n",
    "print(hn_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      "\n",
      "Packed:\n",
      "tensor([[ 0.5373, -0.7590],\n",
      "        [-0.5505,  0.0691],\n",
      "        [ 0.1044, -0.4968],\n",
      "        [-0.3366, -0.1353],\n",
      "        [-0.0494, -0.3800]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Unpacked: Mismatch\n",
      "tensor([[ 0.5373, -0.7590],\n",
      "        [-0.5505,  0.0691],\n",
      "        [ 0.1044, -0.4968],\n",
      "        [-0.3366, -0.1353],\n",
      "        [-0.0494, -0.3800]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 1:\n",
      "\n",
      "Packed:\n",
      "tensor([[ 0.6832, -0.8781],\n",
      "        [-0.4492, -0.2140],\n",
      "        [ 0.2100, -0.6302],\n",
      "        [-0.1983, -0.3963],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Unpacked: Mismatch\n",
      "tensor([[ 0.6832, -0.8781],\n",
      "        [-0.4492, -0.2140],\n",
      "        [ 0.2100, -0.6302],\n",
      "        [-0.1983, -0.3963],\n",
      "        [-0.2015, -0.4695]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 2:\n",
      "\n",
      "Packed:\n",
      "tensor([[-0.0927, -0.9743],\n",
      "        [-0.7976, -0.7854],\n",
      "        [-0.6183, -0.8498],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Unpacked: Mismatch\n",
      "tensor([[-0.0927, -0.9743],\n",
      "        [-0.7976, -0.7854],\n",
      "        [-0.6183, -0.8498],\n",
      "        [-0.1449, -0.2888],\n",
      "        [-0.1950, -0.5119]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 3:\n",
      "\n",
      "Packed:\n",
      "tensor([[ 0.0049, -0.8728],\n",
      "        [-0.7652, -0.2736],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Unpacked: Mismatch\n",
      "tensor([[ 0.0049, -0.8728],\n",
      "        [-0.7652, -0.2736],\n",
      "        [ 0.0844, -0.5717],\n",
      "        [-0.3636, -0.3566],\n",
      "        [-0.1184, -0.5026]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Sample 4:\n",
      "\n",
      "Packed:\n",
      "tensor([[ 0.5847, -0.9850],\n",
      "        [-0.5619, -0.8457],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], grad_fn=<SliceBackward>)\n",
      "\n",
      "Unpacked: Mismatch\n",
      "tensor([[ 0.5847, -0.9850],\n",
      "        [-0.5619, -0.8457],\n",
      "        [-0.1684, -0.2847],\n",
      "        [-0.1837, -0.5158],\n",
      "        [-0.2403, -0.4121]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "for j in range(5):\n",
    "    print(f'\\nSample {j}:\\n')\n",
    "    print('Packed:')\n",
    "    print(test_output_unpacked[:, j, :])\n",
    "\n",
    "    print('\\nUnpacked: Mismatch')\n",
    "    print(output_padded[:, j, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (axiom)",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
