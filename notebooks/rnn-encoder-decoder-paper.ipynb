{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-Encoder-Decoder in Pytorch\n",
    "\n",
    "**|| Jonty Sinai ||** 28-04-2019\n",
    "\n",
    "- **Paper:** [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "- **Authors:** Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio\n",
    "- **Topic:** Machine translation, sequence modelling, Neural network architectures\n",
    "- **Year:** 2014\n",
    "\n",
    "In this notebook I implement a sequence-to-sequence neural network based on the RNN-Encoder-Decoder framework introduced by two seminal papers (above) in neural machine translation (NMT) and deep learning. Both papers were released within a few months of each other and presented at NeurIPS 2014 and as such both are credited with introducing the modern sequence-to-sequence framework. In contemporary deep learning ideas from both papers have been combined and indeed extended into more sophisticated frameworks.\n",
    "\n",
    "I will implement a generic sequence-to-sequence model based on ideas from both papers and will train the model on a simplified English-French translation dataset based on the [official PyTorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py). Whereas in my [ResNet implementation](https://github.com/JontySinai/artificial_neural_networks/blob/master/notebooks/resnet.ipynb) the goal was to explore modular composeability with PyTorch, the goal here is to explore sequence-to-sequence design patterns and engineering with PyTorch.\n",
    "\n",
    "> **Note:** Compared to computer vision, feature selection and data preprocessing in natural language processing (NLP) requires more care and attention. There is no correct way of preprocessing text, although there are many incorrect ways. Choices must be made carefully and efficiently. As a result, a good portion of this notebook is spent preprocessing the dataset and converting it into the right format for sequence-to-sequence learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4e4c14c6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import glob\n",
    "import random\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "HOME = os.environ['AI_HOME']\n",
    "ROOT = os.path.join(HOME, 'artificial_neural_networks')\n",
    "DATA = os.path.join(ROOT, 'data')\n",
    "ENG_FR = os.path.join(DATA, 'english_french')\n",
    "\n",
    "random.seed(1901)\n",
    "np.random.seed(1901)\n",
    "torch.manual_seed(1901)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Model Architecture\n",
    "\n",
    "A **sequence-to-sequence** architecture is a generic type of deep learning architecture consisting of two main components:\n",
    "\n",
    "- An **encoder** which takes in an **input sequence** and encodes it into some **latent representation**, sometimes called the **context vector**.\n",
    "- A **decoder** which takes in the latent representation and produces an **output sequence**.\n",
    "    - The decoder may also take in an input sequence of its own. During training this is typically the target sequence. During prediction, this is the predicted sequence itself, but more on these details later.\n",
    "    - During training the decoder can be thought of as a **generative model** which tries to produce the target sequence, given the context vector.\n",
    "\n",
    "For example, in the French --> English translation task, a sequence-to-sequence model will look as follows\n",
    "\n",
    "<img src=\"assets/seq2seq.png\">\n",
    "\n",
    "Here the output sequence is fed back into the decoder for prediction. \n",
    "\n",
    "An **RNN-Encoder-Decoder** is a type of sequence-to-sequence architecture where both the encoder and decoder are RNN's. The papers presented above have a slight different treatment of how information is passed from the encoder to the decoder. I will summarise both below:\n",
    "\n",
    ">**Note:** We will implement both and compare their performance on the translation task. Note we will be using shallower versions of the LSTM/GRU cells, a smaller dataset and smaller compute, so the conclusions of training in no way reflect which method will be better. In fact, both are valid and contemporary methods use ideas from both. In general it is best to experiment with different architectures for the task at hand.\n",
    "\n",
    "#### Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (Cho et al, 2014)\n",
    "\n",
    "<img src=\"assets/cho-rnn-encoder-decoder.png\" width=\"400px\">\n",
    "\n",
    "- The **encoder** will loop through each timestep in the input sequence exactly as it is with the Sutskever paper.\n",
    "\n",
    "- This time the **context vector** is calculated by passing the final hidden state through a nonlinear layer $f_C$:\n",
    "\n",
    "$$c = f_C(h_n)$$\n",
    "\n",
    "- The **decoder** is kicked off with the `<SOS>` token as input and but now the initial hidden state is calculated using a nonlinear layer, $f_H$, using the contect vector:\n",
    "$$h_0 = f_H(c)$$\n",
    "    <br/>\n",
    "    Then at each time $t$, the hidden state is calculated using the previous hidden state, the decoder input, $\\tilde{y}_{t-1}$ (whose value differs depending on whether the model is being used for training or prediction), and the context vector.\n",
    "    \n",
    "    \\begin{align}\n",
    "    h_t = f_D(h_{t-1}, \\tilde{y}_t , c) \\ , \\ \\ h_0 = c\n",
    "    \\end{align}\n",
    "    <br/>\n",
    "    Finally the output is calculated as a _linear combination_ of the hidden state, decoder input and context vector:\n",
    "    \n",
    "    $$z_t = O_{h}h_t + O_{y}\\tilde{y}_{t-1} + O_{c}c$$\n",
    "\n",
    "- The recurrent cell is a **GRU**, which was introduced in this paper, for both the encoder and decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class choEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding  = nn.Embedding(input_vocab_size, hidden_size, padding_idx=0)  # PAD_token = 0\n",
    "        self.cell = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_, hidden_states):\n",
    "        \"\"\"\n",
    "            input size : max_seq_len x batch_size\n",
    "            hidden_states: tuple (h,c) corresponding to the hidden and cell states\n",
    "                           respectively each with size batch x hidden_size\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embedding(input_)  # size: max_seq_len x batch_size x hidden_size\n",
    "        output, hidden_states = self.cell(embedded, hidden_states)\n",
    "        \n",
    "        return output, hidden_states\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.randn(1, batch_size, self.hidden_size)  # first dim: num_layers * num_directions\n",
    "        c0 = torch.randn(1, batch_size, self.hidden_size)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (axiom)\n",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
