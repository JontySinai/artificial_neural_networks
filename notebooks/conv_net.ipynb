{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network in PyTorch\n",
    "\n",
    "**|| Jonty Sinai ||** 10-04-2019\n",
    "\n",
    "So far I've implemented a simple [one layer neural network](mlp.ipynb) and code for an [n-layer fully connected neural network](neural_network.ipynb) for simple image classification tasks, either MNIST or CIFAR10. With these simple architectures it was possible to get over 97% test accuracy on MNIST (with only a few minutes of training on a CPU) and roughly 51% test accuracy on CIFAR-10 (still way better than a random guess which has 10% accuracy). \n",
    "\n",
    "The main design flaw with these neural networks is that every pixel in the image maps through a unique weight pathway to the final output class probabilities. Put another way, every weight pathway through the network is associated with a unique pixel location in the source image. The problem with this approach is that groups of pixels tend to form similar patterns in images (eg edges, circles, texture etc.) and tend to be repeated across an image. The basic feed-forward architecture of a fully connected network takes no advantage of repeated motifs within an image. This is where convolutional neural networks come in. \n",
    "\n",
    "The idea is to \"look\" at only a subset of the image and systematically cover the image by _striding_ the selected subset. The way we do this is with _convolution_ kernels (matrices with size smaller than the image) which _convolve_ the selected pixels into a small patch of transformed pixels. The learnable weights of the neural network are the weights of the convolution kernels. The advantage of this is that a smaller number of weights can be shared across the image. For RGB colour images we can apply a separate convolution kernel across each colour channel. Convolution kernels are often coupled with _pooling_ layers which summarise the transformed pixel into one hidden unit, usually by taking the average or max value. The overall effect is similar to _compression_, only the compression tends to be lossy because of the pooling layers.\n",
    "\n",
    "<img src=\"./assets/convolution_kernel.gif\" width=\"543\">\n",
    "\n",
    "source: [Rob Robinson, Imperial College](https://mlnotebook.github.io/post/CNN1/)\n",
    "\n",
    "Convolution kernels can be thought of as encoding essential information about the image into a compressed latent representation. With each convolution layer, each pixel in each laten representation covers a wider area of the source image. After several convolution layers, the latent representation can then be passed through a fully connected layer with softmax activation to calculate a distribution over the class labels. By this point, each class probability should reference the entire source image. \n",
    "\n",
    "An example of a convolutional neural network architecture with fully connected output layers is shown below:\n",
    "\n",
    "<img src=\"./assets/cnn_arch.png\" width=\"1000\">\n",
    "\n",
    "source: [Denny Britz, Wild ML](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "Notice how the architecture is flexible enough to accomodate any number of channels in the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fd998f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re  # we'll use this later to process layer type keys in an OrderedDict \n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "HOME = os.environ['AI_HOME']\n",
    "ROOT = os.path.join(HOME, 'artificial_neural_networks')\n",
    "DATA = os.path.join(ROOT, 'data')\n",
    "MNIST = os.path.join(DATA, 'mnist')\n",
    "CIFAR10 = os.path.join(DATA, 'cifar10')\n",
    "\n",
    "random.seed(1901)\n",
    "np.random.seed(1901)\n",
    "torch.manual_seed(1901)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "CNN architectures tend to be wide and varied and a lot of research and experimentation goes into finding the right architecture. One of the original CNN architectures to achieve high success on MNIST was Yann Le Cunn's [LeNet-5](http://yann.lecun.com/exdb/lenet/) from 1998. The architecture is shown below:\n",
    "\n",
    "<img src=\"./assets/le_net_5.png\" width=\"1000\">\n",
    "\n",
    "source: [Andrew Ng, Coursera](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)\n",
    "\n",
    "We'll implement this architecture as part of a more generaliseable convolutional neural network module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    # ref: https://discuss.pytorch.org/t/flatten-layer-of-pytorch-build-by-sequential-container/5983/3\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, arch_dict: OrderedDict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            arch_dict (OrderedDict) : Specifies the CNN archicture where\n",
    "                key, value pairs correspond to layer_type, layer_params.\n",
    "                Layer parameters are specified as a tuple of integers or\n",
    "                they can be None.\n",
    "                \n",
    "                The supported layer types with their parameters are:\n",
    "                \n",
    "                    Conv2d : (in_channels, out_channels, kernel_size, stride, padding)\n",
    "                    AvgPool2d : (kernel_size, stride, padding)\n",
    "                    MaxPool2d : (kernel_size, stride, padding)\n",
    "                    Flatten : None\n",
    "                    Linear : input_size, output_size\n",
    "                    ReLU : None\n",
    "                    Sigmoid : None\n",
    "                    Tanh : None\n",
    "                    \n",
    "                If more layer_types are used repeatedly, then they should be\n",
    "                post-fixed with an underscore followed by an alphanumeric\n",
    "                index. \n",
    "                \n",
    "                Eg: \"Conv2d_1\", \"Conv2d_2\", \"Tanh_1a\", \"Tanh_1b\"\n",
    "                    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # make sure arch_dict is an OrderedDict\n",
    "        # for activation layers, use None for layer_params\n",
    "        for layer_type, layer_params in arch_dict.items():\n",
    "            \n",
    "            layer_type = re.sub(r\"_[\\d\\w]+\", \"\", layer_type) # remove number/letter post-fixing of layer types\n",
    "            \n",
    "            if layer_type == \"Conv2d\":\n",
    "                in_channels, out_channels, kernel_size, stride, padding = layer_params\n",
    "                self.layers.append(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "            elif layer_type == \"AvgPool2d\":\n",
    "                kernel_size, stride, padding = layer_params\n",
    "                self.layers.append(\n",
    "                    nn.AvgPool2d(kernel_size, stride, padding))\n",
    "            elif layer_type == \"MaxPool2d\":\n",
    "                kernel_size, stride, padding = layer_params\n",
    "                self.layers.append(\n",
    "                    nn.MaxPool2d(kernel_size, stride, padding))\n",
    "            elif layer_type == \"Flatten\":\n",
    "                self.layers.append(\n",
    "                    Flatten())\n",
    "            elif layer_type == \"Linear\":\n",
    "                input_size, output_size = layer_params\n",
    "                self.layers.append(\n",
    "                    nn.Linear(input_size, output_size))\n",
    "            elif layer_type == \"ReLU\":\n",
    "                self.layers.append(\n",
    "                    nn.ReLU())\n",
    "            elif layer_type == \"Sigmod\":\n",
    "                self.layers.append(\n",
    "                    nn.Sigmoid())\n",
    "            elif layer_type == \"Tanh\":\n",
    "                self.layers.append(\n",
    "                    nn.Tanh())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer type: {layer_type}\")\n",
    "                \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet-5 architecture can be specified as follows using an `OrderedDict`.\n",
    "\n",
    "> Note the original LeNet-5 architecture specifies zero padding in the first layer and expects 32x32 resolution input images. However MNIST images are 28x28 so we will have to use padding of 2 to keep the fully connected layers consistent with the LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Conv2d_1', (1, 6, 5, 1, 2)),\n",
       "             ('Tanh_1a', None),\n",
       "             ('AvgPool2d_1', (2, 2, 0)),\n",
       "             ('Tanh_1b', None),\n",
       "             ('Conv2d_2', (6, 16, 5, 1, 0)),\n",
       "             ('Tanh_2a', None),\n",
       "             ('AvgPool2d_2', (2, 2, 0)),\n",
       "             ('Tanh_2b', None),\n",
       "             ('Flatten_3', None),\n",
       "             ('Linear_4', (400, 120)),\n",
       "             ('Tanh_4', None),\n",
       "             ('Linear_5', (120, 84)),\n",
       "             ('Tanh_5', None),\n",
       "             ('Linear_6', (84, 10))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet5_arch = OrderedDict()\n",
    "\n",
    "# 1: Convolutional Layer\n",
    "lenet5_arch[\"Conv2d_1\"] = (1, 6, 5, 1, 2)  # Conv layer: 28x28x1 -> 28x28x6, kernel=5x5, stride=1, padding=2\n",
    "lenet5_arch[\"Tanh_1a\"] = None  # followed by tanh nonlinear activation\n",
    "lenet5_arch[\"AvgPool2d_1\"] = (2, 2, 0)  # followed by 2x2 AvgPool, stride = 2, padding=0\n",
    "lenet5_arch[\"Tanh_1b\"] = None  # followed by tanh nonlinear activation\n",
    "# 2: Convolutional Layer\n",
    "lenet5_arch[\"Conv2d_2\"] = (6, 16, 5, 1, 0)  # Conv layer: 14x14x6 -> 10x10x16, kernel=5x5, stride=1, padding=0\n",
    "lenet5_arch[\"Tanh_2a\"] = None  # followed by tanh nonlinear activation\n",
    "lenet5_arch[\"AvgPool2d_2\"] = (2, 2, 0)  # followed by 2x2 AvgPool, stride = 2, padding=0\n",
    "lenet5_arch[\"Tanh_2b\"] = None  # followed by tanh nonlinear activation\n",
    "# 3: Flatten\n",
    "lenet5_arch[\"Flatten_3\"] = None # flatten 5x5x16 -> 400\n",
    "# 4: Fully Connected Layer\n",
    "lenet5_arch[\"Linear_4\"] = (400, 120) # FC layer: 400 input units -> 120 output units\n",
    "lenet5_arch[\"Tanh_4\"] = None  # followed by tanh nonlinear activation\n",
    "# 5: Fully Connected Layer\n",
    "lenet5_arch[\"Linear_5\"] = (120, 84) # FC layer: 120 input units -> 84 output units\n",
    "lenet5_arch[\"Tanh_5\"] = None  # followed by tanh nonlinear activation\n",
    "# 6: Fully Connected Output Layer\n",
    "lenet5_arch[\"Linear_6\"] = (84, 10) # FC layer: 84 input units -> 10 output units\n",
    "\n",
    "lenet5_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Tanh()\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): Tanh()\n",
      "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (7): Tanh()\n",
      "    (8): Flatten()\n",
      "    (9): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (10): Tanh()\n",
      "    (11): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (12): Tanh()\n",
      "    (13): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet5_mnist = ConvNet(lenet5_arch)\n",
    "\n",
    "print(lenet5_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 1, 28, 28)  # 10 batches of 28x28 greyscale (1 channel) images\n",
    "\n",
    "y = lenet5_mnist(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Functions\n",
    "\n",
    "Notice that since we're using convolutions, we don't need to specify the input size parameter to unroll the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, optimiser, loss_function, num_epochs):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch + 1} \" + \"=\"*80 + \">\")\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(training_data):\n",
    "            images, labels = batch\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            output = model(images)\n",
    "            # backward pass\n",
    "            loss = loss_function(output, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # print progress\n",
    "            \n",
    "            if (batch_idx + 1) % 1000 == 0:    # print every 1000 mini-batches\n",
    "                print(\"[%4d/6000] loss: %.3f\" %\n",
    "                      (batch_idx + 1, total_loss / 1000))\n",
    "                total_loss = 0.0\n",
    "                \n",
    "    print(\"Finished Training \" + \"=\"*71 + \">\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_data:\n",
    "            images, truth = data\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += truth.size(0)\n",
    "            correct += (predicted == truth).sum().item()\n",
    "\n",
    "    print('Test accuracy on %d test images: %.4f %%' % (total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "Define dataloaders for [MNIST](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))]  # note that we normalise by rank-1 tensors\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "mnist_trainset = torchvision.datasets.MNIST(root=MNIST, train=True, download=True, transform=mnist_transforms)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "mnist_testset = torchvision.datasets.MNIST(root=MNIST, train=False, download=True, transform=mnist_transforms)\n",
    "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n"
     ]
    }
   ],
   "source": [
    "mnist_classes = tuple(f\"{n}\" for n in range(10))\n",
    "print(mnist_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train LeNet-5 for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_mnist = nn.CrossEntropyLoss()\n",
    "adam_mnist = optim.Adam(lenet5_mnist.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 ================================================================================>\n",
      "[1000/6000] loss: 0.398\n",
      "[2000/6000] loss: 0.156\n",
      "[3000/6000] loss: 0.113\n",
      "[4000/6000] loss: 0.101\n",
      "[5000/6000] loss: 0.091\n",
      "[6000/6000] loss: 0.083\n",
      "Epoch: 2 ================================================================================>\n",
      "[1000/6000] loss: 0.064\n",
      "[2000/6000] loss: 0.068\n",
      "[3000/6000] loss: 0.062\n",
      "[4000/6000] loss: 0.069\n",
      "[5000/6000] loss: 0.057\n",
      "[6000/6000] loss: 0.064\n",
      "Epoch: 3 ================================================================================>\n",
      "[1000/6000] loss: 0.043\n",
      "[2000/6000] loss: 0.050\n",
      "[3000/6000] loss: 0.055\n",
      "[4000/6000] loss: 0.047\n",
      "[5000/6000] loss: 0.054\n",
      "[6000/6000] loss: 0.055\n",
      "Finished Training =======================================================================>\n"
     ]
    }
   ],
   "source": [
    "train(lenet5_mnist, mnist_trainloader, adam_mnist, cross_entropy_mnist, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference the simple MLP achieved a training loss of 0.083 and the 3-layer network achieved a training loss of 0.081 after three epochs. LeNet-5 massively improves the results, beating both shortly into the second epoch and achieving a final training loss of 0.055 after only a few minutes of training on a CPU. This is close to state of the art in 1998 - I only used a computer for the first time in 2000.\n",
    "\n",
    "And now let's evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on 10000 test images: 98.6800 %\n"
     ]
    }
   ],
   "source": [
    "evaluate(lenet5_mnist, mnist_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello world again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "Let's see how LeNet-5 performs when trained on CIFAR-10, which has so far been a tricky dataset for our simple fully connected neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_transforms = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]  # note that we normalise by rank-1 tensors\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_trainset = torchvision.datasets.CIFAR10(root=CIFAR10, train=True, download=True, transform=cifar10_transforms)\n",
    "cifar10_trainloader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=10, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_testset = torchvision.datasets.CIFAR10(root=CIFAR10, train=False, download=True, transform=cifar10_transforms)\n",
    "cifar10_testloader = torch.utils.data.DataLoader(cifar10_testset, batch_size=10, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model. CIFAR-10 images are 32x32 so this time we will use no padding in the first layer. Notice that we also have to increase the number of input channels to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Conv2d_1', (3, 6, 5, 1, 0)),\n",
       "             ('Tanh_1a', None),\n",
       "             ('AvgPool2d_1', (2, 2, 0)),\n",
       "             ('Tanh_1b', None),\n",
       "             ('Conv2d_2', (6, 16, 5, 1, 0)),\n",
       "             ('Tanh_2a', None),\n",
       "             ('AvgPool2d_2', (2, 2, 0)),\n",
       "             ('Tanh_2b', None),\n",
       "             ('Flatten_3', None),\n",
       "             ('Linear_4', (400, 120)),\n",
       "             ('Tanh_4', None),\n",
       "             ('Linear_5', (120, 84)),\n",
       "             ('Tanh_5', None),\n",
       "             ('Linear_6', (84, 10))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet5_arch_cifar10 = OrderedDict()\n",
    "\n",
    "# 1: Convolutional Layer\n",
    "lenet5_arch_cifar10[\"Conv2d_1\"] = (3, 6, 5, 1, 0)  # Conv layer: 32x32x3 -> 28x28x6, kernel=5x5, stride=1, padding=0\n",
    "lenet5_arch_cifar10[\"Tanh_1a\"] = None  # followed by tanh nonlinear activation\n",
    "lenet5_arch_cifar10[\"AvgPool2d_1\"] = (2, 2, 0)  # followed by 2x2 AvgPool, stride = 2, padding=0\n",
    "lenet5_arch_cifar10[\"Tanh_1b\"] = None  # followed by tanh nonlinear activation\n",
    "# 2: Convolutional Layer\n",
    "lenet5_arch_cifar10[\"Conv2d_2\"] = (6, 16, 5, 1, 0)  # Conv layer: 14x14x6 -> 10x10x16, kernel=5x5, stride=1, padding=0\n",
    "lenet5_arch_cifar10[\"Tanh_2a\"] = None  # followed by tanh nonlinear activation\n",
    "lenet5_arch_cifar10[\"AvgPool2d_2\"] = (2, 2, 0)  # followed by 2x2 AvgPool, stride = 2, padding=0\n",
    "lenet5_arch_cifar10[\"Tanh_2b\"] = None  # followed by tanh nonlinear activation\n",
    "# 3: Flatten\n",
    "lenet5_arch_cifar10[\"Flatten_3\"] = None # flatten 7x7x24 -> 1176\n",
    "# 4: Fully Connected Layer\n",
    "lenet5_arch_cifar10[\"Linear_4\"] = (1176, 512) # FC layer: 400 input units -> 120 output units\n",
    "lenet5_arch_cifar10[\"Tanh_4\"] = None  # followed by tanh nonlinear activation\n",
    "# 5: Fully Connected Layer\n",
    "lenet5_arch_cifar10[\"Linear_5\"] = (512, ) # FC layer: 120 input units -> 84 output units\n",
    "lenet5_arch_cifar10[\"Tanh_5\"] = None  # followed by tanh nonlinear activation\n",
    "# 6: Fully Connected Output Layer\n",
    "lenet5_arch_cifar10[\"Linear_6\"] = (84, 10) # FC layer: 84 input units -> 10 output units\n",
    "\n",
    "lenet5_arch_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Tanh()\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): Tanh()\n",
      "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (7): Tanh()\n",
      "    (8): Flatten()\n",
      "    (9): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (10): Tanh()\n",
      "    (11): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (12): Tanh()\n",
      "    (13): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet5_cifar10 = ConvNet(lenet5_arch_cifar10)\n",
    "\n",
    "print(lenet5_cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_cifar10 = nn.CrossEntropyLoss()\n",
    "adam_cifar10 = optim.Adam(lenet5_cifar10.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 ================================================================================>\n",
      "[1000/6000] loss: 1.912\n",
      "[2000/6000] loss: 1.730\n",
      "[3000/6000] loss: 1.640\n",
      "[4000/6000] loss: 1.582\n",
      "[5000/6000] loss: 1.503\n",
      "Epoch: 2 ================================================================================>\n",
      "[1000/6000] loss: 1.441\n",
      "[2000/6000] loss: 1.441\n",
      "[3000/6000] loss: 1.432\n",
      "[4000/6000] loss: 1.405\n",
      "[5000/6000] loss: 1.398\n",
      "Epoch: 3 ================================================================================>\n",
      "[1000/6000] loss: 1.332\n",
      "[2000/6000] loss: 1.334\n",
      "[3000/6000] loss: 1.325\n",
      "[4000/6000] loss: 1.318\n",
      "[5000/6000] loss: 1.318\n",
      "Finished Training =======================================================================>\n"
     ]
    }
   ],
   "source": [
    "train(lenet5_cifar10, cifar10_trainloader, adam_cifar10, cross_entropy_cifar10, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on 10000 test images: 51.6400 %\n"
     ]
    }
   ],
   "source": [
    "evaluate(lenet5_cifar10, cifar10_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 doesn't do as well as hoped on CIFAR-10. However using the flexible CNN architecture module defined above, we'll explore other architectures in the future to try on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (axiom)\n",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
