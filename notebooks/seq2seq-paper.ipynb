{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning in Pytorch\n",
    "\n",
    "**|| Jonty Sinai ||** 28-04-2019\n",
    "\n",
    "- **Paper:** [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- **Authors:** Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n",
    "- **Topic:** Machine translation, sequence modelling, Neural network architectures\n",
    "- **Year:** 2014\n",
    "\n",
    "In this notebook I implement a sequence-to-sequence neural network based on the RNN-Encoder-Decoder framework introduced by two seminal papers (above) in neural machine translation (NMT) and deep learning. Both papers were released within a few months of each other and presented at NeurIPS 2014 and as such both are credited with introducing the modern sequence-to-sequence framework. In contemporary deep learning ideas from both papers have been combined and indeed extended into more sophisticated frameworks.\n",
    "\n",
    "I will implement a generic sequence-to-sequence model based on ideas from both papers and will train the model on a simplified English-French translation dataset based on the [official PyTorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py). Whereas in my [ResNet implementation](https://github.com/JontySinai/artificial_neural_networks/blob/master/notebooks/resnet.ipynb) the goal was to explore modular composeability with PyTorch, the goal here is to explore sequence-to-sequence design patterns and engineering with PyTorch.\n",
    "\n",
    "> **Note:** Compared to computer vision, feature selection and data preprocessing in natural language processing (NLP) requires more care and attention. There is no correct way of preprocessing text, although there are many incorrect ways. Choices must be made carefully and efficiently. As a result, a good portion of this notebook is spent preprocessing the dataset and converting it into the right format for sequence-to-sequence learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcb9817c6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import glob\n",
    "import random\n",
    "import unicodedata\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "HOME = os.environ['AI_HOME']\n",
    "ROOT = os.path.join(HOME, 'artificial_neural_networks')\n",
    "DATA = os.path.join(ROOT, 'data')\n",
    "ENG_FR = os.path.join(DATA, 'english_french')\n",
    "\n",
    "random.seed(1901)\n",
    "np.random.seed(1901)\n",
    "torch.manual_seed(1901)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Model Architecture\n",
    "\n",
    "A **sequence-to-sequence** architecture is a generic type of deep learning architecture consisting of two main components:\n",
    "\n",
    "- An **encoder** which takes in an **input sequence** and encodes it into some **latent representation**, sometimes called the **context vector**.\n",
    "- A **decoder** which takes in the latent representation and produces an **output sequence**.\n",
    "    - The decoder may also take in an input sequence of its own. During training this is typically the target sequence. During prediction, this is the predicted sequence itself, but more on these details later.\n",
    "    - During training the decoder can be thought of as a **generative model** which tries to produce the target sequence, given the context vector.\n",
    "\n",
    "For example, in the French --> English translation task, a sequence-to-sequence model will look as follows\n",
    "\n",
    "<img src=\"assets/seq2seq.png\">\n",
    "\n",
    "Here the output sequence is fed back into the decoder for prediction. \n",
    "\n",
    "An **RNN-Encoder-Decoder** is a type of sequence-to-sequence architecture where both the encoder and decoder are RNN's. The papers presented above have a slight different treatment of how information is passed from the encoder to the decoder. I will summarise both below:\n",
    "\n",
    ">**Note:** We will implement both and compare their performance on the translation task. Note we will be using shallower versions of the LSTM/GRU cells, a smaller dataset and smaller compute, so the conclusions of training in no way reflect which method will be better. In fact, both are valid and contemporary methods use ideas from both. In general it is best to experiment with different architectures for the task at hand.\n",
    "\n",
    "#### Sequence to Sequence Learning with Neural Networks (Sutskever et al, 2014)\n",
    "\n",
    "<img src=\"assets/sutskever-seq2seq.png\" width=\"650px\">\n",
    "\n",
    "- The **encoder** loops through each timestep in the input sequence, $(x_1, x_2, ... ,x_{n})$, updating its hidden state with each step. The final hidden state will form the context vector summarising the entire input sequence. In general if $f_E$ is the encoder, then at each time $t$, the hidden state of the encoder is computed as follows:\n",
    "\n",
    "\\begin{align}\n",
    "h_t = f_E(h_{t-1}, x_t )\n",
    "\\end{align}\n",
    "\n",
    "- The **context vector** is set to the final hidden state of the input sequence:\n",
    "\n",
    "$$c = h_n$$\n",
    "\n",
    "- The **decoder** is started by passing the `<EOS>` token of the input sequence (this is identical to passing a `<SOS>` token) to produce an initial output, $\\hat{y}_1$.\n",
    "    - During training, the true target $y_1$ will be passed to the decoder at time $t=2$, and so on. This is known as **teacher forcing**.\n",
    "    - During prediction, the estimate $\\hat{y}_1$ is passed to the decoder at time $t=2$, and so on.\n",
    "    - The process is repeated until the decoder predicts an `<EOS>` token, marking termination of the generative procedure. \n",
    "    <br/><br/>\n",
    "    In general if $f_D$ is the decoder, then at each timestep $t$, the hidden state of the decoder is computed as\n",
    "    \n",
    "    \\begin{align}\n",
    "    h_t = f_D(h_{t-1}, \\tilde{y}_t ) \\ , \\ \\ h_0 = c\n",
    "    \\end{align} \n",
    "    <br/><br/>\n",
    "    where $\\tilde{y}_t = y_{t-1}$ during training and $\\tilde{y}_t = \\hat{y}_{t-1}$ during prediction, with $\\tilde{y}_1 = \\text{<SOS>}$.\n",
    "    <br/>\n",
    "    The output is then calculated using a linear layer:\n",
    "    \n",
    "    $$z_t = Wh_t$$\n",
    "\n",
    "- The recurrent cell is an **LSTM** for both the encoder and decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sutskeverEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding  = nn.Embedding(input_vocab_size, hidden_size, padding_idx=0)  # PAD_token = 0\n",
    "        self.cell = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_, hidden_states):\n",
    "        \"\"\"\n",
    "            input size : max_seq_len x batch_size\n",
    "            hidden_states: tuple (h,c) corresponding to the hidden and cell states\n",
    "                           respectively each with size batch x hidden_size\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embedding(input_)  # size: max_seq_len x batch_size x hidden_size\n",
    "        output, hidden_states = self.cell(embedded, hidden_states)\n",
    "        \n",
    "        return output, hidden_states\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.randn(1, batch_size, self.hidden_size)  # first dim: num_layers * num_directions\n",
    "        c0 = torch.randn(1, batch_size, self.hidden_size)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Sutskever Encoder\n",
    "\n",
    "Let's now see how a random tensor with the right dimensionality is transformed through the forward pass of the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sutskeverEncoder(\n",
      "  (embedding): Embedding(10, 8, padding_idx=0)\n",
      "  (cell): LSTM(8, 8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "seq2seq_encoder = sutskeverEncoder(10, 8)  # input_vocab_size=10, hidden_size=8\n",
    "\n",
    "print(seq2seq_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.randint(size=(3, 5), low=0, high=10) # max_seq_length = 3, batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "embedded = seq2seq_encoder.embedding(input_)\n",
    "\n",
    "print(embedded.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8])\n",
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = seq2seq_encoder.init_hidden(5)  # batch_size = 5\n",
    "\n",
    "print(hidden_states[0].size())\n",
    "print(hidden_states[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "output, hidden_states = seq2seq_encoder.cell(embedded, hidden_states)\n",
    "\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden_states = seq2seq_encoder(input_, hidden_states)\n",
    "\n",
    "encoder_hidden, _ = hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sutskeverDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding  = nn.Embedding(output_vocab_size, hidden_size, padding_idx=0)  # PAD_token = 0\n",
    "        self.cell = nn.LSTM(hidden_size, hidden_size)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, output_vocab_size)\n",
    "        \n",
    "    def forward(self, input_, hidden_states):\n",
    "        \"\"\"\n",
    "            input size : max_seq_len x batch_size\n",
    "            hidden_states: tuple (h,c) corresponding to the hidden and cell states\n",
    "                           respectively each with size batch x hidden_size\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embedding(input_)\n",
    "        output, hidden_states = self.cell(embedded, hidden_states)\n",
    "        output = F.softmax(self.linear(output), dim=1)\n",
    "        \n",
    "        return output, hidden_states\n",
    "    \n",
    "    def init_hidden(self, context, batch_size):\n",
    "        c0 = torch.randn(1, batch_size, self.hidden_size)  # first dim: num_layers * num_directions\n",
    "        return context, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test: Sutskever Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sutskeverDecoder(\n",
      "  (embedding): Embedding(12, 8, padding_idx=0)\n",
      "  (cell): LSTM(8, 8)\n",
      "  (linear): Linear(in_features=8, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "seq2seq_decoder = sutskeverDecoder(12, 8)  # output_vocab_size=10, hidden_size=8\n",
    "\n",
    "print(seq2seq_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.randint(size=(3, 5), low=0, high=10) # max_seq_length = 3, batch_size = 5\n",
    "hidden_states = seq2seq_decoder.init_hidden(context=encoder_hidden, batch_size=5)\n",
    "\n",
    "output, hidden_states = seq2seq_decoder(input_, hidden_states)\n",
    "\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (axiom)\n",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
