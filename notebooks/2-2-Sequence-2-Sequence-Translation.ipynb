{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation with Sequence-to-Sequence Models in PyTorch\n",
    "\n",
    "Sequence-to-sequence models are a general class of recurrent neural networks which map an input sequence to an output sequence. The high-level architecture is based on a classic idea in information theory where an input signal is processed by an _encode_, _transmitted_ and then _decoded_ by another model to produce an output signal.\n",
    "\n",
    "In sequence-to-sequence models the _encoder_ and _decoder_ are recurrent neural networks (or some variant) themselves. The basic idea generalises to a wide variety of architectures, as long as they are end-to-end differentiable.\n",
    "\n",
    "Here I will follow this [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py) except to translate English sentences into French sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d14eed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import glob\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "HOME = os.environ['AI_HOME']\n",
    "ROOT = os.path.join(HOME, 'artificial_neural_networks')\n",
    "DATA = os.path.join(ROOT, 'data')\n",
    "ENG_FR = os.path.join(DATA, 'english_french')\n",
    "\n",
    "random.seed(1901)\n",
    "np.random.seed(1901)\n",
    "torch.manual_seed(1901)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "English-French sentence pairs can be downloaded [here](https://download.pytorch.org/tutorial/data.zip) (the original data comes from [https://tatoeba.org/eng/downloads](https://tatoeba.org/eng/downloads) and has been paired thanks to [https://www.manythings.org/anki/](https://www.manythings.org/anki/). \n",
    "\n",
    "Once again we'll need to do the hardwork of preprocessing this data into tensors which we can feed into our model. Since this is a sequence-to-sequence model, we'll need to be careful to ensure that pairs of input-output sequences are aligned during training.\n",
    "\n",
    "The data is stored on one file with each line representing an English-French sentence pair. Each English and French sentence is separated by a tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Encoding and Normalisation\n",
    "\n",
    "The files are all in unicode which we will simplify to ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(string_):\n",
    "    # ref: https://stackoverflow.com/a/518232/2809427\n",
    "    return ''.join(\n",
    "        char for char in unicodedata.normalize('NFD', string_)  # Unicode normal form decomposition of character\n",
    "        if unicodedata.category(char) != 'Mn')  # non-spacing mark characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Wow!\tCa alors !\n",
      "Fire!\tAu feu !\n"
     ]
    }
   ],
   "source": [
    "sentences = open(os.path.join(ENG_FR, 'eng-fra.txt'), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "for line in sentences[:5]:\n",
    "    print(unicode_to_ascii(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be helpful to normalise the strings to lowercase and remove non-letter characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(string_):\n",
    "    string_ = unicode_to_ascii(string_.lower().strip())\n",
    "    string_ = re.sub(r\"([.!?])\", r\" \\1\", string_)\n",
    "    string_ = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", string_)\n",
    "    return string_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . va !\n",
      "run ! cours !\n",
      "run ! courez !\n",
      "wow ! ca alors !\n",
      "fire ! au feu !\n"
     ]
    }
   ],
   "source": [
    "for line in sentences[:5]:\n",
    "    print(normalize_string(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Indexing\n",
    "\n",
    "The English and French sentences need to be indexed for conversion to tensors. However we'll need to be careful not to mix up the indexing between the two languages. To save time and ensure that preprocessing scopes are kept local to each language we'll implement a class to manage preprocessing for each.\n",
    "\n",
    "Each vocabulary will be a dictionary map between words and a unique index representing their one-hot-encoding index.\n",
    "\n",
    "In sequence-to-sequence models we also use two special tokens to indicate the start and end of sentence. These will be the first two tokens in our dictionary mapping for each language.\n",
    "\n",
    "<img src=\"assets/word-encoding.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # only have SOS and EOS so far\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words  # use total count to index new word\n",
    "            self.index2word[self.n_words] = word \n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1 \n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this class to maps words to indices and indices back to words.\n",
    "\n",
    "## File-Vocabulary I/O\n",
    "\n",
    "Next we'll define a function to read in language pairs and create vocabularies. We've seen earlier that the sentence pairs are in the order `English -> French`. If we want to reverse the translation we can use a reverse flag to change which language is the input language and which is the output language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(path, in_lang, out_lang, reverse=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (str): Path to language file in form: lang1-lang2.txt.\n",
    "        in_lang (str): Name to use for input language.\n",
    "        out_lang (str): Name to use for output language.\n",
    "        reverse (bool): If true reverse the roles of in_lang and out_lang.\n",
    "        \n",
    "    Note:\n",
    "        The sentence pairs are always parsed as in_lang -> out_lang,\n",
    "        unless reverse is true.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read the file and split into lines\n",
    "    sentences = open(path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    \n",
    "    # split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in sentences]  # [[en_1, fr_1], [en2_, fr_2], ... ]\n",
    "    \n",
    "    # init vocabularies\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_vocab = Vocab(out_lang)\n",
    "        output_vocab = Vocab(in_lang)\n",
    "    else:\n",
    "        input_vocab = Vocab(in_lang)\n",
    "        output_vocab = Vocab(out_lang)\n",
    "\n",
    "    return input_vocab, output_vocab, pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Filters\n",
    "\n",
    "To make training quick and to reduce model size and complexity we'll restrict sentences to a maximum sequence length (including ending punctuation).\n",
    "\n",
    "We'll also filter to the specific case translating to sentences of the form \"He is ...\", \"She is ...\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep(pair, max_seq_length, reverse=False):\n",
    "    eng_sent_prefixes = (\"i am \", \"i m \",\n",
    "                    \"he is\", \"he s \",\n",
    "                    \"she is\", \"she s \",\n",
    "                    \"you are\", \"you re \",\n",
    "                    \"we are\", \"we re \",\n",
    "                    \"they are\", \"they re \")\n",
    "    if reverse:\n",
    "        eng = pair[1]\n",
    "        fr = pair[0]\n",
    "    else:\n",
    "        eng = pair[0]\n",
    "        fr = pair[1]\n",
    "        \n",
    "    condition =  len(fr.split(' ')) < max_seq_length and \\\n",
    "                 len(eng.split(' ')) < max_seq_length and eng.startswith(eng_sent_prefixes)\n",
    "    \n",
    "    return condition\n",
    "\n",
    "def filter_pairs(pairs, max_seq_length, reverse=False):\n",
    "    return [pair for pair in pairs if keep(pair, max_seq_length, reverse)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Putting Everything Together\n",
    "\n",
    "We'll use one function which handles all the preprocessing (this could also be implemented as a class with greater engineering sophistication and complexity), covering:\n",
    "\n",
    "- read sentences, split into pairs and init vocabs\n",
    "- filter pairs by length and content (if they start with certain prefixes)\n",
    "- encode each pair as ASCII and normalise\n",
    "- add new words to each respective vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_pairs(path, in_lang, out_lang, max_seq_length, reverse=False):\n",
    "    \n",
    "    input_vocab, output_vocab, pairs = read_vocab(path, in_lang, out_lang, reverse)\n",
    "    print(f\"Total number of {in_lang}-{out_lang} sentence pairs: {len(pairs)}\")\n",
    "    \n",
    "    pairs = filter_pairs(pairs, max_seq_length, reverse)\n",
    "    print(f\"Total number of pairs after filtering: {len(pairs)}\")\n",
    "    \n",
    "    print(\"Adding words to respective vocabularies ...\")\n",
    "    for pair in pairs:\n",
    "        input_vocab.add_sentence(pair[0])\n",
    "        output_vocab.add_sentence(pair[1])\n",
    "    \n",
    "    print(\"Total number of words:\")\n",
    "    print(f\"\\tInput: {input_vocab.name}: {input_vocab.n_words}\")\n",
    "    print(f\"\\tOutput: {output_vocab.name}: {output_vocab.n_words}\")\n",
    "    \n",
    "    return input_vocab, output_vocab, pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of English-French sentence pairs: 135842\n",
      "Total number of pairs after filtering: 10599\n",
      "Adding words to respective vocabularies ...\n",
      "Total number of words:\n",
      "\tInput: English: 2803\n",
      "\tOutput: French: 4345\n"
     ]
    }
   ],
   "source": [
    "input_vocab, output_vocab, pairs = preprocess_sentence_pairs(\n",
    "                                        path = os.path.join(ENG_FR, 'eng-fra.txt'), \n",
    "                                        in_lang = \"English\", \n",
    "                                        out_lang = \"French\", \n",
    "                                        max_seq_length = 10, \n",
    "                                        reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to Tensors\n",
    "\n",
    "Now that we've done preprocessing, we will need to convert each sentence in each vocabulary to tensors. PyTorch's `nn.Embedding` layer does not require onehot vectors as input. Instead we can just pass a vector of indices corresponding to each word in the sequence. Normally we would restrict this vector to a maximum sequence length and pad each each sequence with zeros for batching effiency in a PyTorch `Dataloader`. \n",
    "\n",
    "### Torch Dataset\n",
    "\n",
    "We'll use a Torch Dataset to handle conversion of the data using the vocabs and pairs defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_vocab, output_vocab, pairs):\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.tensor_pairs = []\n",
    "        \n",
    "        self.add_tensors(pairs)\n",
    "        \n",
    "    def add_tensors(self, pairs):\n",
    "        for pair in pairs:\n",
    "            input_tensor, output_tensor = self.pair2tensors(pair)\n",
    "            self.tensor_pairs.append((input_tensor, output_tensor))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        input_tensor, output_tensor = self.tensor_pairs[index]\n",
    "        return input_tensor, output_tensor\n",
    "        \n",
    "    def sentence2sequence(self, vocab, sentence):      \n",
    "        return [vocab.word2index[word] for word in sentence.split()]\n",
    "    \n",
    "    def sentence2tensor(self, vocab, sentence):\n",
    "        sequence = self.sentence2sequence(vocab, sentence)\n",
    "        sequence.append(self.EOS_token)\n",
    "        \n",
    "        return torch.tensor(sequence, dtype=torch.long).view(-1, 1)  # seq_len x 1 tensor\n",
    "    \n",
    "    def pair2tensors(self, pair):\n",
    "        input_tensor = self.sentence2tensor(self.input_vocab, pair[0])\n",
    "        output_tensor = self.sentence2tensor(self.output_vocab, pair[1])\n",
    "        return input_tensor, output_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensor_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10599\n"
     ]
    }
   ],
   "source": [
    "english_french_dataset = TranslationDataset(input_vocab, output_vocab, pairs)\n",
    "\n",
    "print(len(english_french_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect what our tensors look like. We'll look at a sentence pair at an arbitrary index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:  torch.Size([5, 1])\n",
      "Output sequence:  torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "input_tensor, output_tensor = english_french_dataset[42]\n",
    "\n",
    "print(\"Input sequence: \", input_tensor.size())\n",
    "print(\"Output sequence: \", output_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2],\n",
      "        [ 3],\n",
      "        [33],\n",
      "        [ 4],\n",
      "        [ 1]])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the English vocab to convert this sentence back into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i m sure . EOS\n"
     ]
    }
   ],
   "source": [
    "sentence = ' '.join(input_vocab.index2word[idx.item()] for idx in input_tensor.view(-1))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2],\n",
      "        [14],\n",
      "        [11],\n",
      "        [50],\n",
      "        [ 5],\n",
      "        [ 1]])\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j en suis certain . EOS\n"
     ]
    }
   ],
   "source": [
    "sentence = ' '.join(output_vocab.index2word[idx.item()] for idx in output_tensor.view(-1))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Finally let's define a dataloader for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_at = int(0.8 * len(pairs)) \n",
    "train_indices = range(split_at + 1)\n",
    "test_indices = range(split_at, len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_trainloader = DataLoader(english_french_dataset, sampler=SubsetRandomSampler(train_indices), num_workers=0)\n",
    "en_fr_testloader = DataLoader(english_french_dataset, sampler=SubsetRandomSampler(test_indices), num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-Encoder-Decoder\n",
    "\n",
    "The first type of sequence-to-sequence model we will look at is the [RNN-Encode-Decoder](https://arxiv.org/abs/1406.1078), also known as [seq2seq](https://arxiv.org/abs/1409.3215) (both papers appeared at roughly the same time and ideas from both are used in modern sequence-to-sequence models).\n",
    "\n",
    "The RNN-Encoder-Decoder is split into two parts, an _encoder RNN_ and a _decoder RNN_. \n",
    "\n",
    "<img src=\"assets/seq2seq.png\">\n",
    "\n",
    "The **encoder** loops through the input sequence, updating its internal memory of the input over time, until it encounters the `<EOS>` token. At this point the last _hidden state_, known as the _context vector_ is **transmitted** to the **decoder**. The decoder works slightly differently during training and prediction.\n",
    "\n",
    "- At training time, the **decoder** is provided with the `SOS` token at $t_0$ and tries to predict the next word. At the next time step, $t_1$, the correct word is fed as input to the decoder which then tries to predict the next word at time $t_2$ and so on. This is known as **teacher forcing**.\n",
    "- At prediction time, the **decoder** is once again provided with an `SOS` token at $t_0$, however the prediction made for $t_1$ is then fed back into the network at time $t_1$ in order to make a prediction at time $t_2$. This process continues until the decoder has predicted an `EOS` token.\n",
    "\n",
    "Note that using the `SOS` and `EOS` tokens, there is no need for the input and output sequences to align or have the same length. This makes the sequence-to-sequence architecture highly flexible.\n",
    "\n",
    "### Encoder Module\n",
    "\n",
    "We'll start by constructing the encoder network which behaves like a normal RNN, taking the current input and previous hidden state and passing them throught a GRU or LSTM cell. Since the sequence space is discrete, we will use an embedding layer to transform each onehot vector encoding into a continuous vector representation.\n",
    "\n",
    "<img src=\"assets/encoder-network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, cell='GRU'):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # map inputs to same dimensionality as hidden states\n",
    "        \n",
    "        if cell == 'GRU':\n",
    "            self.cell = nn.GRU(hidden_size, hidden_size)\n",
    "        elif cell == 'LSTM':\n",
    "            self.cell = nn.LSTM(hidden_size, hidden_size)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported cell type {cell}!')\n",
    "            \n",
    "    def forward(self, input_, hidden):\n",
    "        embedded = self.embedding(input_).view(1, 1, -1)  # hidden states are 1x1xhidden_size tensors\n",
    "        output, hidden = self.cell(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Module\n",
    "\n",
    "The decoder module is slightly more complex. We use a relu activation function between the input and RNN cell as an additional nonlinearity and a softmax layer for the output.\n",
    "\n",
    "<img src=\"assets/decoder-network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size, hidden_size, cell='GRU'):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        \n",
    "        if cell == 'GRU':\n",
    "            self.cell = nn.GRU(hidden_size, hidden_size)\n",
    "        elif cell == 'LSTM':\n",
    "            self.cell = nn.LSTM(hidden_size, hidden_size)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported cell type {cell}!')\n",
    "            \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        embedded = F.relu(self.embedding(input_)).view(1, 1, -1)\n",
    "        output, hidden = self.cell(embedded, hidden)\n",
    "        output = self.softmax(self.linear(output[0]))  # output is 1x1xhidden_size and we need 1xhidden_size\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_through_time(input_, target, encoder, decoder, encoder_optimiser, \n",
    "                          decoder_optimiser, loss_function, max_seq_length,\n",
    "                          teacher_forcing_ratio, use_attention=False):\n",
    "    input_length = input_.size(0)\n",
    "    target_length = target.size(0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # zero gradients\n",
    "    encoder_optimiser.zero_grad()\n",
    "    decoder_optimiser.zero_grad()\n",
    "    \n",
    "    # encoder forward\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    if use_attention:\n",
    "        encoder_outputs = torch.zeros(max_seq_length, encoder.hidden_size)\n",
    "    \n",
    "    for idx in range(input_length):\n",
    "        encouder_output, encoder_hidden = encoder(input_[idx], encoder_hidden)\n",
    "        if use_attention:\n",
    "            encoder_outputs[idx] = encouder_output[0, 0]\n",
    "        \n",
    "    # decoder forward\n",
    "    decoder_hidden = encoder_hidden  # context vector passed to decoder\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    for idx in range(target_length):\n",
    "        if use_attention:\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "        else:\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "        if use_teacher_forcing:\n",
    "            # feed target as next input\n",
    "            decoder_input = target[idx]\n",
    "        else:\n",
    "            top_vals, top_idxs = decoder_output.topk(1)\n",
    "            # feed predicted indices as next input\n",
    "            decoder_input = top_idxs.squeeze().detach()  # detach from history as input\n",
    "            # if reached EOS_token, then stop using predictions\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                loss += loss_function(decoder_output, target[idx])\n",
    "                break\n",
    "        \n",
    "        loss += loss_function(decoder_output, target[idx])\n",
    "    \n",
    "    # --- backpropagate through time ---\n",
    "    \n",
    "    # if decoder predictions <EOS> rightaway, then loss is type int \n",
    "    \n",
    "    # notice that autograd will differentiate through the accumulated sum of loss in the for loop\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimiser.step()\n",
    "    decoder_optimiser.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, training_set, learning_rate, num_epochs, print_every, plot_every,\n",
    "          max_seq_length, teacher_forcing_ratio, use_attention):\n",
    "    \n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    encoder_optimiser = optim.SGD(encoder.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    decoder_optimiser = optim.SGD(decoder.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    print(\"\\nTraining \" + \"=\"*81 +\"\\n\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch + 1} \" + \"=\"*80 + \">\")\n",
    "        \n",
    "        running_loss_plot = 0\n",
    "        running_loss_print = 0\n",
    "        \n",
    "        for idx, batch in enumerate(training_set):\n",
    "            input_, target = batch\n",
    "            \n",
    "            if len(input_) < 1:\n",
    "                continue\n",
    "            \n",
    "            input_ = input_.view(-1, 1)  # seq_len x batch_size\n",
    "            target = target.view(-1, 1)\n",
    "            \n",
    "            loss_item = backprop_through_time(input_, target, encoder, decoder, encoder_optimiser, \n",
    "                          decoder_optimiser, loss_function, max_seq_length,\n",
    "                          teacher_forcing_ratio, use_attention=False)\n",
    "            \n",
    "            running_loss_print += loss_item\n",
    "            running_loss_plot += loss_item\n",
    "            \n",
    "            if (idx + 1) % print_every == 0:\n",
    "                print_loss_avg = running_loss_print / print_every\n",
    "                running_loss_print = 0.\n",
    "                percent_all_epochs_done = ((epoch * len(training_set)) + idx + 1) / (len(training_set) * (num_epochs))\n",
    "                print('%s (%d/%d) %.4f' % (timeSince(start, percent_all_epochs_done),\n",
    "                                             (idx + 1), len(training_set), print_loss_avg))\n",
    "\n",
    "            if (idx + 1) % plot_every == 0:\n",
    "                plot_loss_avg = running_loss_plot / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                running_loss_plot = 0.\n",
    "    \n",
    "    print(\"Finished Training \" + \"=\"*71 + \">\")\n",
    "    showPlot(plot_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18m 18s'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "epoch = 0\n",
    "idx = 1000\n",
    "((epoch * len(en_fr_trainloader)) + idx + 1) / (len(en_fr_trainloader) * (num_epochs))\n",
    "# ((num_epochs * len(en_fr_trainloader)) + idx + 1) / (len(en_fr_trainloader) * (num_epochs + 1))\n",
    "\n",
    "es = 45 / (((epoch * len(en_fr_trainloader)) + idx + 1) / (len(en_fr_trainloader) * (num_epochs)))\n",
    "rs = es - 45\n",
    "asMinutes(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-Encoder-Decoder - Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_size=input_vocab.n_words, hidden_size=128, cell='GRU')  # input_vocab_size = 2803\n",
    "decoder = DecoderRNN(output_size=output_vocab.n_words, hidden_size=128, cell='GRU')  # input_vocab_size = 4345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training =================================================================================\n",
      "\n",
      "Epoch: 1 ================================================================================>\n",
      "0m 41s (- 16m 44s) (1000/8480) 3.9771\n",
      "1m 20s (- 15m 39s) (2000/8480) 4.0990\n",
      "2m 5s (- 15m 37s) (3000/8480) 3.9181\n",
      "3m 4s (- 16m 28s) (4000/8480) 3.9322\n",
      "3m 46s (- 15m 25s) (5000/8480) 3.4938\n",
      "4m 30s (- 14m 37s) (6000/8480) 3.3350\n",
      "5m 17s (- 13m 57s) (7000/8480) 3.1784\n",
      "6m 6s (- 13m 18s) (8000/8480) 3.0622\n",
      "Epoch: 2 ================================================================================>\n",
      "7m 15s (- 12m 13s) (1000/8480) 2.8294\n",
      "8m 6s (- 11m 34s) (2000/8480) 2.7220\n",
      "8m 51s (- 10m 46s) (3000/8480) 2.8321\n",
      "9m 36s (- 9m 59s) (4000/8480) 2.7874\n",
      "10m 24s (- 9m 13s) (5000/8480) 2.7971\n",
      "11m 15s (- 8m 30s) (6000/8480) 2.6859\n",
      "12m 2s (- 7m 45s) (7000/8480) 2.7293\n",
      "12m 47s (- 6m 57s) (8000/8480) 2.6684\n",
      "Epoch: 3 ================================================================================>\n",
      "13m 56s (- 5m 48s) (1000/8480) 2.3974\n",
      "14m 42s (- 5m 1s) (2000/8480) 2.3308\n",
      "15m 34s (- 4m 16s) (3000/8480) 2.4467\n",
      "16m 24s (- 3m 30s) (4000/8480) 2.4772\n",
      "17m 18s (- 2m 44s) (5000/8480) 2.3984\n",
      "18m 18s (- 1m 58s) (6000/8480) 2.4585\n",
      "19m 5s (- 1m 10s) (7000/8480) 2.3826\n",
      "19m 59s (- 0m 23s) (8000/8480) 2.3791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4nFeZvu8zvWjUJVuW5JbYcRw7cRynkV4gJJRQQlt6C7DA0pey7MLCNuBHWWCBpS11gdBCyKaQ5oQQUuwU23Hvki2rl9H0cn5/fGWKRtJIGsmW8t7X5UujmW8+nU9Knu+d57znOUprjSAIgrCwcJzsAQiCIAiVR8RdEARhASLiLgiCsAARcRcEQViAiLgLgiAsQETcBUEQFiAi7oIgCAsQEXdBEIQFiIi7IAjCAsR1sn5wY2OjXr58+cn68YIgCPOSrVu39mmtmyY77qSJ+/Lly9myZcvJ+vGCIAjzEqXUkXKOE1tGEARhAVKWuCulDiultiulnlZKjSm3lcHXlVL7lVLblFIbKz9UQRAEoVymYstcpbXuG+e164FV5r8LgW+bXwVBEISTQKVsmRuBn2iDR4FapVRLhc4tCIIgTJFyxV0Df1JKbVVK3Vzi9VagI+/7TvO5ApRSNyultiiltvT29k59tIIgCEJZlCvul2itN2LYL+9VSl1e9Loq8Z4xu4Borb+rtd6ktd7U1DRpJ48gCIIwTcoSd631cfNrD/B74IKiQzqB9rzv24DjlRigIAiCMHUmFXelVFApFbIeAy8AdhQddhvwJrNr5iJgWGvdVfHRAntOhPnyn/bQN5qYjdMLgiAsCMqp3BcBDyulngEeB/5Pa32XUurdSql3m8fcARwE9gPfA/52VkYLHOgd5Rv376d/NDlbP0IQBGHeM2krpNb6IHBOiee/k/dYA++t7NBK43QY9n4qk52LHycIgjAvmXcrVN1OQ9zT2THztYIgCILJvBN3l8MYciYrlbsgCMJ4zENxt2wZqdwFQRDGY/6Ju9Oq3EXcBUEQxmPeibtMqAqCIExO2eKulHIqpZ5SSt1e4rWlSqkHzNe3KaVuqOwwc1gTqlK5C4IgjM9UKvcPALvGee3TwC1a63OB1wLfmunAxsMpnrsgCMKklJvn3ga8CPj+OIdooNp8XMMsRg+4Tc89Ld0ygiAI41JunvvXgL8HQuO8/lmM1Mj3A0Hg2lIHmYmSNwMsXbp0SgO1sLplxJYRBEEYn3KyZV4M9Gitt05w2OuAH2mt24AbgJ8qpcacuxKpkFafu9gygiAI41OOLXMJ8FKl1GHgl8DVSqmfFR3zduAWAK31XwEf0FjBcdq47AlVsWUEQRDGY1Jx11p/UmvdprVejjFZer/W+g1Fhx0FrgFQSp2JIe6zshuHLGISBEGYnGn3uSulPqeUeqn57UeAd5rJkb8A3mKGiVUcaxFTWvrcBUEQxmUqG2Sjtd4MbDYf/1Pe8zsx7JtZxyXBYYIgCJMy71aoWraMiLsgCML4zENxl2wZQRCEyZiH4i7ZMoIgCJMx78Td4VA4lFTugiAIEzHvxB2MjhlphRQEQRifiqRCmq+/Wim1Uyn1rFLqfys3xLG4HEpaIQVBECZgKq2QVipkdfELSqlVwCeBS7TWg0qp5gqNryQuh5JuGUEQhAmoVCrkO4H/0loPAmiteyozvNK4nA5JhRQEQZiAcm0ZKxVyPEVdDaxWSv1FKfWoUuqFpQ5SSt2slNqilNrS2zv9dAKXQ8mEqiAIwgRUKhXSBawCrsRIiPy+Uqq2+KBKpEKCIe4yoSoIgjA+lUqF7AT+oLVOaa0PAXswxH5WcDkdUrkLgiBMQKVSIW8FrgJQSjVi2DQHKzxWG5dTySImQRCECahUKuTdQL9SaifwAPAxrXV/JQZYCqMVUip3QRCE8ahUKqQGPmz+m3VcDoe0QgqCIEzAPF2hqqQVUhAEYQLmp7hLK6QgCMKEzE9xdzpkQlUQBGEC5qe4S+UuCIIwIfNT3CUVUhAEYUIqlgppHnOTUkorpTZVZnilMYLDxJYRBEEYj6lU7lYqZEmUUiHg74DHZjqoyZA+d0EQhImpVCokwOeBLwLxCoxrQoxWSBF3QRCE8ahIKqRS6lygXWs9rmVjHlehVEjJlhEEQZiIGadCKqUcwFeBj0x2roqlQkq2jCAIwoRUIhUyBKwDNpvHXATcNpuTqpXy3EcT6Rmfw7rJbD0yyNYjAzM+nyAIQiWYcSqk1npYa92otV5uHvMo8FKt9ZbZGrSxE9P0xV1rzVf+tId1n7mbB/ZMf9OonpE46z5zN1sOD/CFu3bzH3funva5BEEQKkmlUiHnlJm2Qm7e28vX798PQOdAdNrn6RiMkkhnOdgbYSSWIprMTPtcgiAIlaQiqZBFx1w500FNhsvhIDMDW+bEcK6hJzwDa2Y4lgJgJJ4iHE/jdc/LNWGCICxApiTupwpupyI1g8o9v8IOx2cu7uF4mtFEGq2d0z6XIAhCJZmX4u6cYbZMPGWIe8jrIhxPTfs8w9Fc5T6aSONQ0z6VIAhCRZmXPoKVLWPsETJ1YskMToeiLuiZYeVuvLcnnCCT1cRS4rkLgnBqMD/F3SyRp1q9H+qL0DEQJZbK4Hc7CflcFbFljg/FAIinsmRLjKlrOMYtT3RM++cIgiBMlfkp7k5D3KfSDqm15qr/t5lrv/Ig0WQGny3uM7BlTHHvGspN0CbSY+cCfvfkMf7+t9uISTeNIAhzREVSIZVSH1ZK7VRKbVNK3aeUWlbZYRbidhjDnoq4b+scBgzxjacyBDxOQj53RSr37nBO3OMlrBlrsVRSVtUKgjBHVCoV8ilgk9b6bOA3GAFis4bTsmWm0A75f9u7AFhc7SOWrIwtM2KKe771X8p3typ2iUwQBGGuqEgqpNb6Aa21tRroUaCtMsMrjdu0ZabSDvl/2wxxdzoU0VQGn8dJtc/NSAVsmXxKiXvErNwlplgQhLmiIqmQRbwduLPUC5VKhXSatky5E6rRZJpj5qRnJJkmnszgdzsI+VyMJtL2JGjXcIw3//DxkqJdipLiXsJXj6akchcEYW6ZcSpk0bFvADYBXyr1eiVTIaF8sRyIJAGoD3qIJjLEUhkCHhchnwutDcEHeKZjiAf39rLnRLis85YS91KeuyX44rkLgjBXVCIVEgCl1LXAP2CEhiUqOsoirFbIcm2OwYghwu11fpKZLCPxlOm5u4HcKlXLUomUEUmQTGeJpTI0VnkLni9ly0STYssIgjC3zDgVEuzNOv4bQ9inH7NYJi7n1LplBqJG5d5WFwCgL5ywWyEhT9yTRmVdTt6MVbW31fkLno+nxlbnUZlQFQRhjqlUKuSXgCrg10qpp5VSt1VkdOPgtir3MidUB01bpr3eEPdIMtcKCdi97lOp3McT99KVu9gygiDMLRVJhdRaX1vRUU2Cc4q2zIAt7jkh9ntKVe7G16mJu3HDCHqcRJIZ4iUmVC3PXWwZQRDminm5QtU9RVtmMJrEoaClxmc/53M7qTbFfaSoci9nh6aRosq9KeQtOEc+1oSt2DKCIMwV81Lc7UVMZdoyA5EkdQGPbcMApSdUTc99tIyFTVbl3lqGuIvnLgjCXDMvxT3XCll+5V4X9BD05FyoQClbxvLck5OL+4kRI3Lg9KYqALtrJpbMEE2muf4//8yWwwNkspqkmTdT7ngFQRBmyrwUd9uWmYLnXh/wUOXNibvf7cTvduJ0KHtCNW7bMpMHfHUORqkNuFlsWj01fjdel4N4OsPRgSi7ukZ4pnPYboMEqdwFQZg7KhUc5lVK/UoptV8p9ZhSanklB1mMc8rdMinqgm4C3txOST6PE6UUdQEPvWGjLd+a+CxnQrVzMEZbnR+300FjlYfmah9+j5N4MkNf2JjADccL91UVcRcEYa6oVHDY24FBrfXpwFeBL8x0YBNhp0KWW7lHk9QHx1buAGsWh9htrki1YgLK8dw7B2O01RqdMr//20t41+Ur8budxFIZ+kaNm0U4ni4Sd7FlBEGYGyoSHAbcCPzYfPwb4Bql1KxtOper3CcXS601g+aEqtflsN8b8BjivnZJNXu6w6QyWbuNcbJuGa01nYNRu7WyvT5A0OsyxT1ri/toPC22jCAIJ4VKBYe1Ah0AWus0MAw0zHh04+B2lm/LhBNp0llNfdCDUoqgKeo+s3Jf21JNMp3lYG+k7AnVvtEk8VTW7nG38LmdxJIZeq3KPZEqCBJLi7gLgjBHVCo4rFSVPqasrlQqpGsKE6rW6tS6gAeAoGnNWLbM2iXVAOzsGi65QjWT1XQMRMmnc9D4vnh1qs/tIJ7K99zTRPLEPSm2jCAIc0SlgsM6gXYApZQLqAEGik9UsVRIRy4V8kh/ZMIUR8siqa8qEnezgl/ZGMTjcrDz+IhdZedv4PGun27hsi8+UGCvdAwa8cFWnIGF3zPWc4/lvU8qd0EQ5oqKBIcBtwFvNh/fZB4za2VqgynUJ4bjfPrWHXz0188UvN4TjturTjtNIW6rNapsS9wtz93ldLBmcYhdXWG7FTKRzpLOZDkxHOfeXUYOmhVhYJzTqNxbawsrd7/baVTutrhLt4wgCCeHSgWH/QBoUErtBz4MfKISgxuPgMdFe72fPd1hdhwbLhBegLf96Ak++dvtALalkp8BAznP3XjNT9dwjGgyY38qiCQyfPehg/YxI7FcBX5sMEZdwG3fKCx8JbplxJYRBOFkUKngsDjwqkoObDJWN4d49GA/g9HUmBbDo/1RDvZGSKQzdAzEaKzy2jZMsecO0BD00jfaTyyVYXG1jxMjcUaTabYeHbSPyd+ObyCSHJPjbp0zmsjQP5rz3MWWEQThZDAvV6gCrF4cos8U0dFE2hbORDrDiNlf/tjBATryWhYBqrwuXA6Fx5W79Pqgx86KaQwZlk84nmJ/d5jzl9cBhbsuDUSMOINi/B4nJ0bipLOa2oCbWCpj+/del0NsGUEQ5oz5K+6Lqgq+t3rT8y2a+3f3GOKe17IY8rnsKt7C8vABmsyKfF/3KJFkhvOW1QO5FEgwsmrqA6XF3WJFYxCAnpEEXpfDFHexZQRBmBvmrbivag4VfG954pYl4nM7uGdnN11D8YLK/U0XL+eLrzy74L31eVW4Zbc8dXQIwK7cR/I6aAYiqZKV+8vPbbUfr2gwxL07HCfgceKRyl0QhDlk3or76c1VOFRuQdPuEyO8+jt/taMEXrmxjWNDMdJZXbDY6PTmKq5f31JwroZgzj9vNKN7nzT99o1L61AqZ8torY3KPeimmDWLq/niK8+mNuBmw9JaALpHEgQ8LlwOEXdBEOaOeSvuPreT11+4jNeevxSAB/b08vjhAe7acQKAN1y0zO6MaS9aSVpMKVtmW+cQzSEvdWYmjWXLjMTTZLLaXhRVzKvPb+epf3w+KxsN26hrOEbA48TtUrITkyAIc0Y5K1R9SqnHlVLPKKWeVUr9c4ljliqlHjBTI7cppW6YneEW8vmXreP1FxnifqBnFICnzIq7vT7ADWaFnm/LlCLflmmr81MbcJPVsMr09Wv8brtbxlrxWl/ClrFQStlZ8UPRFMsbg7idDtlDVRCEOaOcVsgEcLXWelQp5QYeVkrdqbV+NO+YTwO3aK2/rZRaC9wBLK/8cMdSbe6mtL/XEPf+SBKvy0HQ4+TvrllFa52fpfUTV+51AQ9KgdaGaP/hvZfwzfv3c+UZzfbPsCr3fivOYAJxB2xxB1jfWsPR/qjYMoIgzBnlrFDVWutR81u3+a/YX9BAtfm4BjhesRFOQrXfEPf8LpnGKi9KKdrrA3zw2tVMFlDpdCjbZvF7nCxrCPKlV53Di85uMX+Gy56wtSv3cWwZi/wt/da1VhfYMg/u7WVb59BULlMQBGFKlBv561RKPQ30APdorR8rOuSzwBuUUp0YVfv7KzrKCQh6nHaMr0W+h14uls2Sv7jJosbvtidUB6KT2zJQWLmva63B5cjZMp/63Xa+ft/+KY9REAShXMoSd611Rmu9AWgDLlBKrSs65HXAj7TWbcANwE+VUmPOXalUyKJzUu0rdJcaJhHeUtji7hkr7tW+sZ77ZLaM1+XA7VQ0h7w0h3x4nEa3TCKd4fhwjOFYcsL3C4IgzIQpdctorYcw4gdeWPTS24FbzGP+CviAxhLvr0gqZDGWNWNVyw0logEmo7Fq/Mq92p/z3AeiSTxOh92JMx7GpKqb9a01ALYt0zkYQ2tjolUQBGG2KKdbpkkpVWs+9gPXAruLDjsKXGMecyaGuFemNC8Da1J10zJjwdFMbBnfOLZMJJkhlckyGEnaG39MxideuIb3XHkagN3nfrTfCDIbiom4C4Iwe5TTLdMC/Fgp5cS4Gdyitb5dKfU5YIvW+jbgI8D3lFIfwphcfctsRv4WU2NW7heubOAv+/vH7JBUDqsXhWgOefG6xt7vLNsnHE+Puzq1FK8+v91+7HYa8QNH+iOAsShKa13WTUIQBGGqTCruWuttwLklns9PhdyJsanHSaHab1zG6U1V3PnBy8bskFQOb7hwGa/e1F5SbC3b5xePH2XHsWFOaw5O+fxupzI2FzEjiJPpLPFUtqTHLwiCMFPm7QrVfCxbprnay2lNVXhdUxdMh0OVtGTyz/+lu/cA8Ipz26Z8frc5oZq/Zd+wWDOCIMwSU8pzP1WxbJnmkG9Wzr+o2jjv5aub+J+3nD+m9bIccrZMFJdDkc5qhmJJFtfMzpgFQXhusyDE/bSmKppCXrvjpdKsa63mzg9cxhmLQjimIexg2DLJTJa+gQSrF4XY2TXCsHTMCIIwSywIcX/VpjZevrEVl3N2XCalFGe2VE9+4AS4nQ4GIkkyWc2ZLdXs7BqRjhlBEGaNBSHuSik7+vdUxe10kMkaDUQrm4wJWfHcBUGYLSqSCmke92ql1E7zmP+t/FDnN/k3H2uXpmJb5jsPHuD7fz6IIAjCTKlIKqRSahXwSeASrfWgUqp5lsY7b3HnWUZL6wM4HWpM5f7HZ46jFLzjspVzPTxBEBYYlUqFfCfwX1rrQfM9PRUd5QLAlVe51wc9VPtcDBXly4TjaU4Mx3n2+DDP/8qDMuEqCMK0qVQq5GpgtVLqL0qpR5VSxdkzz3nyK/f6oIfagIfhWLrgmHA8Rd9okof29rGvZ5QDfaPFpxEEQSiLSqVCuoBVwJUYCZHft/Jo8pmNVMj5gscU94DHic/tpNrvZiiaq9y11vYm3FuPDAC5BEpBEISpUqlUyE7gD1rrlNb6ELAHQ+yL3z8rqZDzAcuWsQLKavOSJgFiqYzdTfPEYWOrwAERd0EQpkmlUiFvBa4yj2nEsGmk7SMPy5axxL3G7y7ocw/HcxaNNdE6GBVxFwRhepRTubcADyiltgFPYHjutyulPqeUeql5zN1Av1JqJ/AA8DGtdf/sDHl+YrVCWtv51Qc9BZV5OD528nQgUvhcTzjOO3+yRewaQRAmpVKpkBr4sPlPKIFVuVu7RDUEPYTjaRLpDF6Xc8zkKoz13LccHuSend08dHYvN25onf1BC4Iwb1kQqZDzAUvcrSx4a7eoQbM6L1m5F9ky/abYP9MxPGvjFARhYSDiPke4iyZUra/7e0Z5yTcetgW7KWSIfmOVZ0zlPjBqfL/92NCcjFkQhPnLgsiWmQ8UT6haCZYP7u1h+7FhrD1CVi+qojec4Oy2Wg6buzZZDEQSAOw4NkI6kyWSyBBJpllSO/XNSQRBWNiIuM8Rti0TKLRltnUaFfve7jAAL9vQSkPQS7XfxVNHBwvOYdkysVSG/b2j/PiRIzxxeIB7P3zFnFyDIAjzB7Fl5oi2Oj9el4NVi6qAXAX/7PERAOKpLE6H4qbz2vj6686lPuBhKJaye9/B6Hu3JmS3dQ5zYjjGob4I6Ux2jq9GEIRTnYqlQprH3qSU0kqpTZUd5vxnZVMVuz//Qk5rMsS92ufC7VSMJnJdMiGfy97DtS7oQWsKFjoNRJKsb6sBoGckzrAp/idG4nN4JYIgzAfKqdytVMhzgA3AC5VSFxUfpJQKAX8HFOfOCCb5m28rpWgIegteD/lyLplV2ed3zPRHkiyu9hHwOBmKpuy4gs7B2GwOWxCEeUilUiEBPg98EZAyskwsAbcIed32Y8ubtzpmtNYMRpLUBz3U+N0Mx1J2VX9MxF0QhCIqkgqplDoXaNda3z4LY1ywNJgdM0GPE4Bqf4nK3RT3kViadFbb4j4US9kxBVOt3KPJNBs/fw/37+6e8TUIgnBqMuNUSKWUA/gq8JHJzvNcToUsRaPZMbNxWR0AIV9e5W6Ku9Uh02+2QVri3hNOkEgbE6mdg9Ep/dz+0SQDkST7uiVSWBAWKpVIhQwB64DNSqnDwEXAbaUmVZ/LqZClsKrz82xxz1XuVh98X9gQdauCN7Lg3XQO5AQ9v3K/f3c3jxzom/DnRpMZgILJXEEQFhaT9rkrpZqAlNZ6KC8V8gvW61rrYaAx7/jNwEe11lsqP9yFhWXLnL+8HoDqvMrd63JS43fTO2qIu1XBNwS91Po99vdup6JzKCf0X7xrD/VBD887zf6TjCGaNEQ9P4lSEISFRaVSIYVpcG57Hac1BdnQXsv61hrWtdYUvN5Y5aG3uHKv8lATyN0EVi8K0TUUt3vd+0aTDE2yPV9MKndBWPBUJBWy6PkrZz6s5wYXn9bAfR+5EoA/vv/SMa83hbz0WZW7+bXB9Nwt1rZU8+zxEbrDCVqqfQxGk3hdDrJZTedgjKUNgTHntWyZUmFlkUQav9uJw6HGvCYIwvxBVqiewjSFfHblvqsrTGutH5/bWSjuS6oBox3SWtQ0HEvxp53dXPXlzRwfGttJE02VrtxTmSyXfuF+btnSMVuXJAjCHCHifgqTb8s80znEhnZjW9raQGHlDkbHjNVRM5pIc6B3lExWs79nbEdM1BT10XjxBt1pBqMp9pV4jyAI8wsR91OYppCXSDJDx0CUzsEY57QbnnytP7f4aY0t7jH6R3OrWQ/2GomSRwbGtknatkyiWNwNm8aygARBmL+IuJ/CNJl98PftMhYbndNmVO6WLeNxOajxu2kOeekcjBZs27e/16i+O0qIe8yyZUpU7pDrzBEEYf4i4n4KY23ccd/uHhwKu5vGsmUskW+r89M5GKMvT5QPmtbK0f4oI/GU3SEDuVbIYs/d+r5vVMRdEOY7FUmFVEp9WCm1Uym1TSl1n1Jq2ewM97mFtYL10YP9rF4UIug1mpusVshqc9FTW12AzsGYvVMT5CyXQ30Rnv+VBzn7n+/mC3ftBnK2TDSZKYgLtip5sWUEYf5TqVTIp4BNWuuzgd9gBIgJM6TZrNxTGc3Lzs1tiF3lceFQUJ1XuR8fitE7OjazbU93mO6RBE6HYvMeI/Ihv4qPJHKPwwnDcx+IJMlmS2XDCYIwX6hIKqTW+gGttWXuPoqRQSPMkPqgB6XA63Lwmk3t9vMOh6LG77ZXtLbVBUhnNbu6wgVJkz638ef1OB1cdUaznSIZTY4VdMhV7umsZqRED7wgCPOHiqRCFvF24M5KDO65jsvpYEVDkJvOa7ODxCza6gK01vnNx8bXrUcGWd4QsPdjPducgL34tAYW1/jyxD3ntef77vndM+K7C8L8pqw9VLXWGWCDUqoW+L1Sap3WekfxcUqpNwCbgJKbeiqlbgZuBli6dOm0B/1c4g/vuwSf2znm+f956/n282e31eBzO4insjSFvFT7jLz3C5bX8/ihAZ6/dhF9ownCiTSZrC6o3AcjKVKZLG6noyBrpn80QVZrvnbvXr76mg14XWPHIAjCqUslUiEBUEpdC/wD8FKtdckZOUmFnDohn9veXDufxiovVeYEa23Aw03nGU6Y1rlumgtW1PPrd1/Ma89vty2ccDxFNJkhZL7373/7DG/6weNAYWtkfyTJXw/0c8f2EyUXQgmCcGpTTrdMk1mxk5cKubvomHOB/8YQ9p7ZGKgwMW+7ZAUAVV6X3SLZUOXh/OX1uJwO+7nhmNEW2VxtTNZ2DMQ40m8seBo1c2XAqNwty0Z2ehKE+UelUiG/BFQBv1ZKPa2Uum2WxiuMw8qmKn797ov5p5estYXcaqWEXGfNSCxNNJWmOeSzX7O89nA8zdJ6I2isbzRp2zTHivJpHj3Yzwd/+VSBdy8IwqlFRVIhtdbXVnhcwjSwcuFrzf1XrX1YIdcTby1osip3MJIgtdaE4ylqAm7qAm47pwYKK/e7dpzg3T/bCsDbLl1hT9oKgnBqIStUFyBLan201PjwuHJ/Xmvh03AsRSSRobHKa3fVZLURSTCaSBPyumio8tI/mrR74PMr99ueOWY/zu+RFwTh1ELEfQHy/qtX8Zv3PK/gOWtCdSiaIpbKEPS6qPLkPriNxtOGuPtc1AXcDEVTJW2ZZzqGbesmIpt9CMIpi4j7AqTK66K11l/wnOXD94SNVawBj5PGkNeeQB1NpAnH01T5XNT4PQzFUoyaC5wsW6Z/NMGxoRgXr2wAICKeuyCcsoi4P0cIeJw4HYoTwzlx/96bNvHZl64FDHEfjaep8rqpDbgZjibtbpn+SJJYMsP2Y8MAPO90U9zFlhGEU5ayFjEJ8x+lFNU+FydGDHH3u52c3lyV28YvkiSZyRLyuUhlsgzFUnhcDlwORTqr2bynh8cPD6CU0T8PYssIwqnMpOKulPIBDwFe8/jfaK0/U3SMF/gJcB7QD7xGa3244qMVZkSN351XuRt/emshlPV8yOcia65iHYgkWdEYZF/PKO/5+ZMAnNYUtNsoZYNtQTh1Kadyt1IhR5VSbuBhpdSdWutH8455OzCotT5dKfVa4AvAa2ZhvMIMqPa7OdJv5LsFPIbXbol7lynuVV4X1tbYI/E0r9jYyPkr6lnbUk3faIIzW6pxOhR+t1Mqd0E4hSmnz10DE6ZCAjcCnzUf/wb4plJKme8VThFq/EbmDIDfFHcrI747T9xdeXEHDUEP779m1ZhzBb0uIsnSnntvOEF90IPToUq+Ppfs7wlTG/AULOgShOcClUqFbAU6ALTWaWAYaKjkQIWZY7VDQq57JmQubuoYNCr6ar/bfg2gylf6/l/lLV25x5IZrvzSA/zqiY6KjXsmvPVHT/C1e/ee7GEIwpxTlrhrrTPDvRKRAAAgAElEQVRa6w0YOe0XKKXWFR1SqkQbU7UrpW5WSm1RSm3p7e2d+miFGWEtatq4tJY1i0OAkRXvdCgOmHuuttT4qM0Xd29pcQ94XCXFvTecIJLMsLc7XOnhT5lsVtM1FGc4JvaR8NyjUqmQnUA7gFLKBdQAAyXeL6mQJxErC+bjL1yDMpenKqWo8rroHjG6ZhZV++xUSchV9sVUeV0l+9wHokYO/PGh2Qsbe/dPt/JPfxiTOD2GkXiKdFaTSJ16LZuZrObf7tjFob7IyR6KsECpSCokcBvwZvPxTcD94refenz6RWv52ms2cOHKQsfMqs7rgx58bie1fk/ea25KEfQ6S/a5D5iZNNYEbaXRWvOX/X080zk86bHWhiOJdHaSI+eew/0RvvvQQe7d2X2yhyIsUMrplmkBfqyUcmLcDG6xUiGBLVrr24AfAD9VSu3HqNhfO2sjFqZNe32AdjM6IB9L3BdVGy2OIZ8LpYxs+PE896DXZXfe5NNvCmrX8OxU7idG4oQTaXpHJr95WBt9J9KnXuWeH7MsCLNBpVIh48CrKjs0Ya6wBLylxhB3h0PZuzmN57kHPaVtmUHTlukbTRJPZUruIjUT9nUbcwM94QTZrMYxQUdOf+TUrdytG+NU20k7B6NoTcmbtCDkI/EDgt0Oubgml/Fu+e7jee5Br6ukLWMJKuQWRk2FTFbzwJ4exnP19pm7QqWz2r6RjIdduadOXXGfauX+qd/v4O9/s202hiQsMETcBXvLvZbqPHE3O2bGq9yrvE4iyfQYER7ME/fjedbM8aHYuIKdzyMH+njr/zwxrqe+L68LpydccjdHm5znPvYmdKgvQncZ1s5scXiatkxvOGFHSAjCRIi4CwS9hnWyKK9yr/a7USq3knXse1xoTcFm2wADkaT9nq4hQ4R2HBvmki/cz2OHjAaq3SdG+O3WzpLnHTBvDsXCe3woxmVfvJ87tnfZSZaTibu14UgpW+aV336EC//tPjoGxs4bzAWTVe7//eABfr1l7FqBkVjK/kQiCBMh4i7YHTEtBbaMx4giUKU97YBZ0Rf77gORJGe2VAO5SdU/7exGa2wh/fc7dvOJ320jkx1byVtWz0Ck0HLZ2x2mYyDGSDxtB5f1TFLB9k/QLWOd/73/++SE5xiIJOkaLu9TR7mkM1n7dzGe5/7TR49w2zPHxzw/Ek8xEk+Typx6VpNwaiHiLlBlVu754n7RynquWD3+WgTrPcW++0AkyZJaP/VBD8fMyv3BvcaCteFYiuFoir/s7yOV0Xa2fD6W2BWL+1DUiE04b1kd77xsJVBG5W6Je1Gfu1Ute1wOdhwbJmmK/y1bOrhzexdZ86bTN5rgwn+7l4v//X4+d/vOgnPcub2rwIKaCseH4qTNn2FtiKK1tgVba03PSGLMp6JsVttjn2y+QRDK6XNvV0o9oJTapZR6Vin1gRLH1Cil/qiUesY85q2zM1xhNlhS6yfgcdJSk9vg4/UXLuObf7Nx3PcEzVTJ4sqzP5KkPuDm9KYqdhwbpn80wbbOIcAQpD/tPGELW+fg2HZJO0PeFObOwSg7jg3bYva9N23i0lWNhHwuekuI+77uMF+/bx8dA1H68myZfGG0JnovXFFPVhs/I57K8PHfbuM9P3+Sf7jVWCB1oGeUVEbTVufnV090EI4bN5jBSJL3/PxJvv/wwfF/qRNw1KzaW2p89iefW58+xqZ/uZd4ykjjTGayY3634UQa6wNE8c1PEIopp3JPAx/RWp8JXAS8Vym1tuiY9wI7tdbnAFcCX1ZKeRDmBTed18bmj15pd82UgzXR+tcD/bziW3/hvf/7JKlMlnA8TX3QyxVnNLH92DC/3tppC9JQNMXdz3bbnnnn4Fi/O1e5JxiIJLn0Cw/w4m88zGA0hVK5TJxF1b6SE6Lf3nyAr9yzl6u/vJmDvcakZSKd5Yd/OcS6z9xNTzhui/tF5mKuI/1ROgaMFkOPy8FD5icN6+bzsevOIJrM8Lsnjf1jrQz8Jw4Nlv37ymfEvEm01wXsTz737uphOJZiIJK0J0yLK/cRM/TN+P2IuAsTM6m4a627tNZPmo/DwC6MoLCCw4CQMgzaKoyFTLI6Y57gcjpozuuUKQfLc//XO3bxVMcQd+84YUcO1AfdtqXz5T/t4fTmKlY0BhmKpjjYN8ol5k5OnQNjK3erkh2IpvjU77bbzw9Fk1T73HbSZHPIa9sy6UyWF3z1Qb7z4AEePdjPZasaaQgWpkD+ZX8fALc9fdwWT0vcD/dH7AnOy1c1cmwoRiSRtsPUXrhuMee01/KLx48CuXbPpzuHprVAyrqBNVV7GTVtmSePGDeKcDxt37SiRfMZ1k0BRNyFyZmS566UWo6xoKk4FfKbwJnAcWA78AGttcz4LGAszx3g8zeuI53V3GMupa8PejlrSTVNIS+pjOY9V5xGfdDDUCxJ70iCtroATSHvOLaMNaGa4KF9uXC5owNR6vIyb5pDXlsEN+/pZW/3KP/1wH6OD8d5wdpF/Psr1gPG5iLG8cbN67dPHuOEOdG7tqWakNfF4b6I3Zp4zZmLANjfM0rnYIxF1V68LievOLeV3SfC7O8J2157Mp1l+zgtm7dvO86Hf/V0yddi5hxAc8hLMpPlcF/EjmsIx1OcGDZuWsXzGSN5AWjT9fuF5w5li7tSqgr4LfBBrfVI0cvXAU8DS4ANGHnu1SXOIamQC4SmKkMs33rJcm7csASHgl9vMdobG6s8KKV40foWVjYFeemGJdT63XQNGdEBzdVe2ur8dA6Nb8vs7xklmsywcWktYKxMrQ3knL61S6rpHIxxqC/Cr7Z0oFRucvKilQ1ctaaZhz52FW+8aBmQCzTb1TXC5j291Abc+D1OljUGONwf5ehAlJDPZXfi7OsZpWMgSnudsRL0+nWLUQpu39ZVsFDricODbN7Tw1X/bzPD0Vxlff/uHm59+hjpEl0tlt1i3XCsCWcwrsH6ZBFLZeyOoof39RVMonYMxvjhw4fs8x8birG/5+QncQqnDuXmubsxhP3nWuvflTjkrcDvtMF+4BCwpvggSYVcONQE3Oz45+v4zEvOIuRzs661hj3dYU5vrmLjsjoA/vHFa7nzA5fhdjqoDXjs6ripyktbXWDCCdW4uar0ghWGdXJsKFZQud+4oRWHMmyf+3f38OaLlxPyuWgIeji9uQqApQ0B2z4ajCSp8rpwKNhyZJDFpg21rCHIkf4Ih/ujLGsIsKw+gMfpYF9PmM7BGG11xiRzc7WPC5bXc/u2LtsSaavz89TRQR450M+hvgi3b8+1LvaGE2R14Ypdi2gijVLQUGXcrB7KE/eReMreOAUMgT/YO8obfvBYQUb+zx49wudu32l/urnqS5u59isPTfg3E55blNMtozCCwXZprb8yzmFHgWvM4xcBZwDTayUQ5g35q1cvNCvez7xkLW5zJyenQ+F1GfZNbcCN1dbeXO2jrc7P8aEYmazRAmi1HxZ3iFjnBajLq9wXVfu4fHUTt2/roi7g5l1XrORTN5zJB5+/uqA332tm2A9Gk7TV+TnPvPFYUQvLG4ybzIGeUZbVB3E5HaxsCrKrK0zXcIy2ulyGy1VrmtnfM8qhvgghn4v1rTXs7xm1J26tCVfIrY4tNekbTWbwu532yuCnOoZorTVuIiPxNN15LaLRRNq+QTxjdh01Vnns6v+hvcZcQtKs4IekRVIwKadyvwR4I3C1Uupp898NSql3K6XebR7zeeB5SqntwH3Ax7XWfbM0ZuEU5F1XnMZ333gel60q/Yms2C9vq/OTymi6R+Lc8J9/5r8e2A8UirvH6eCc9lr7+3xbBuAdl66krc7PD958Pi01fl53wVLbhrGwbi6D0RRBr4vnrzU8datyX99aQzqrOTYUY1mDIeSnN1fx2MF+shra63PtoSsaDf/+6Y4hGoIeVjVXcbg/wu4TIzgUbD0yaKc9Wh01Vk5+PpFkhoDHZQe2DUSSnLXEcDENzz1ecKw16TpkdgwtzQsNy6/6AXYcK3ZMhecq5XTLPKy1Vlrrs7XWG8x/d2itv6O1/o55zHGt9Qu01uu11uu01j+b/aELpxKNVV5ecNbicV+vyRPm5pDXFsonDg+wr2fUzpIZTWRoDhmdLksbAtQF3PYOUvk3CIBLVzXy8MevLrgBFON1G+8dihqxCNeaE6ZWT/91Zy22n7PE/QVnLbZXteZX7tbrh/oi1Ac9nNZcZfbJx+yJ2KeODpHJajsioFTlHkumCXicBa2nqxeFcDmU3S1j/Q4iiXRBl0yV12XvB7u8IcDBvggdA1GWmJ9EHj88wA8ePnRKxhzPd3Z1jdifMOcDskJVmBMsYXY5FHUBD2csMrb5++MzXYDhqYMhZlZlurwhiFKKJlPM6oJTXzph2TJZbeTkrGyq4tuv38jfXLgUMHai+n+vOps3XbyMq85oBuCl5yzhK68+hzWLQ6xtyfUFtOcJfX3Qw6rmkP291fp5dCDKYDRpW1ClIhKMyj1ny4BxI6v2u+kfTTAYTbHS7PKJJjMF+TPVPjf15u/h/VcbG5c/dmgAr7l24Nub9/P523fyyIH+Kf+uhPHpGIhy/X/+mXt3zZ/NVUTchTnB2t2pKeTF4VA0VHlprPLYtsKxwSiZrCaWythZ5ZbANZlVbF1gOuKea9m0VtVev77FPicYds/nblxX0Ov/io1t3PXBywtuKMG8qrk+6GFlUxArTv7MlmqaQ146BqIFK2dL2TIxU9zzK/el9QFCPhf7zUjjlU3GpHAkmba7gMAIdDu9uYpF1V6uWmPcjIaiSWKmB5/KGHeVYyUmq+cjlcj0iacyvOV/HmfPiel3E1lRGcdmcfvISiPiLswJVj58c56onrE4ZE8EjuS1AJ7eXMUZi0JccnpjwXuKbZlysCp3gIB35huHLDU9+PqgF5/baX/KWNkYpL0+QMdg1PbblaJgctQimkwXeO7GeQ1xtzYjWWnaVtFEznMHqPa5eNslK9j80auoNt8fTWaIJo0OHL/bidOh5pUIjcfWIwOc+U93zXhXr0N9ETbv6eXWp49NfvA4WGsM5tPiMRF3YU6wxD2/Yl69KFRwzF4zq70u4OHuD11uWx3We4onVMshfycoq3KfCcsaDNFtMCv6VYtC1AXc1AU9tNf56RiI2eK+ojFYsnKPWpW7OR6P08Giah/VPjdh04I5raByz3nu1X43DofC73HicjrwuhxEkmliqQw3X7aSv37yapbU+mZ1g/LZYCiaNCMgcpX6U0eHiKeyM54ktkLnHjcjp6eDNe8h4i4IRVjC3BTKWR9rFocKvlofm4NFFbZtywRnVrn7x8mmnwqWZWTZNR95wWq++poN9mtdwzF7tem6JTUlPXdL3J0ORcDjpK3Oj9OhCna9sj33RJpwIo3bafg/1b7C30HQ62IkliKV0QS9LmoDHpbU+Dk2GONbm/fzhxlUq9NlOJYad+VuKTbv6WHD5+7hsi8+UDBXYMUiH+6LzHg8ANs6h2z7aqqMxBdg5V5OKqR53JVmm+SzSqkHKz9UYT4T9DhZ1VzFuXmdLVblbnnHe01xL9796eo1zbz47BZ7YnUqWN0yxhgqULmb4m5V7msWV3OlORHbXhcgq2FbxzBel9Ev3x9J2pHCFtFk2l5cFfS67BtGyBRuv9tpb1YeSWYIx9Msbwjidir7E5BFwOOkN5y0HwO01vk5OhDli3ft4QO/LB2BMJv86C+HedV/P1K2X26tE4BCT9tKzzw4Q3G3AtdSGc1THdMMezPPUWpR2qlKRVIhlVK1wLeAl2qtz0I2yxaKUEpxz4ev4NXnt9vPnd1Wy4euXc3bLlmB1+VgT7dVuReK8NlttXzzbzbick79g2b+hGolPPcLV9ZzTlsNZ7WOSdewRfrJo4M0VnntXvp3/2xrwY5P0WSGgGkXvfb8dl6x0cjhsyr3xTU+vC4HLocimkwzGk9TG3DznTecx1uet7zgZwY9LnvHKcuCaq31T5p1P5v0RxLEU1l7lfFk5E8Y588vHK1Q5T4UywnydK2Zkby45/lCpVIh/wYjfuCoeVxPpQcqLDycDsUHrl1FU8hLa53f9tzH27d1OuTbMpWo3NvqAvzhfZfauTD5WAueesIJmkJG3//rL1zKYwf7+cgtz5DNarJmR5BVZX/kBWdw4wbjfyfLcllU7UUpw7KJJDKEEylCPjfXnLnIvoHY1+R12h6/XbnX5hZe5c9xzBWWWBfv0jUeo4kUHvPGbb03m9V0mB0/hypgyzgditZav33DmCoLfkJ1glTI1UCdUmqzUmqrUupNlRme8FxhWX3AbuObSq78ZFTac5+Ilho/NX4jlvjy1U3UBz3868vX85mXnMXjhwd48Tce5lub96N1LjI5H7tyNyv+oNdlV+7j3fCCXpe9sYmVk99alxP3+mlMQs8US6DL9bfD5icTv9vJaMKokHtHEyTTWRZX+zgxEp+2Vw7GhGqN301jlceOhZgq9kYt0eS8WchU9v9Fk6RCuoDzMPJl/MBflVKPaq33Fp3jZuBmgKVLl85k3MIC4xUb23hgj9HzXjyhOhOUUnhcDpLpbEUq94lwOhT3f+QKvG5ngRi/alMbRwYi/GZrJ999yIhcKrXxuF25m6tNAx6n7bnnT7bmE/A47ZwZ6+a1JK9yj6bmflsFS6DLrdzDiTRVPheaXHCcVWFfvrqRW7Z0crg/Yu/NO1WGYylq/W4aqrwlVwyXgzWhmtXG+aazoG6uqVQqZCdwl9Y6YmbKPAScU3yQpEIK43H9ulx0QSVtGchV75Xw3Cejoco7ZvxKKT523Rpu3NBqi4TfXULc/cb7FoXyKnezW6ZqHHHPv2H58zz3uoAbl0PNqOKdLpZAF+8kNR7GzctNyOeyfz9Hrc1TzHbYmVgzw7EU1XblPr25iPxdsObLpGqlUiH/AFymlHIppQLAhRjevCCUhcvp4PM3nsW5S2tLCt9MsCZVZ7tyn4x8L7yU9WR1yyzOq9wHoymS6WxBVEE++ecJmNfnczt57FPX8ubnLT854m4KdDRRrrinCHldhLwu+71HB6IoBZeaC9lmKu61AaNy7x9NTmvV60g8ZW/xOF9894qkQmqtdwF3AduAx4Hva613zNqohQXJGy9ezu//9pKCyN5KYFfus+y5T0a+uJfy/89uq+GVG9u42Nz+L+hx2X3yIV/pHv/8TyN+T+5/Z4/LYVg2qUxFlvBPRPH5Lc+9eJvA8Rg1bacqn8v2tvsjCeoCHmoDHppDXrtj5smjg3zit9umdE05z91LOqvtvvepYLSjGpPZC0bcy0mFNI/7ktZ6rZkK+bXZHbYglI/V637SxT1vorPUp4iQz82XX32O7ecGvC47kmHcCdV8W6bonH6PE62xEy7HYySemvYN4JH9faz45B121DFgr7Kdmi3jIuR125bOYDRl9/SvaAzalfu9O7v55RMdBVsOTobluTeam6NMZ1J1JJZiuRkJsWDEXRDmO7YtU2Evf6rki3s5N5oqr9NOl5xoQtWi2M6yeumjSaN6v2dn95ht//pGE5z/L/dy/+7pdS8/sMd4348fOQJAIp2xF22VK+6jiTRVXjdVvpwtMxw1BBkMcbd28bKENb93/ddbOvjMH0obBdmsti0VK/Rtqr57PJUhkc7a0RMDkdLvH4ompz1hOxuIuAsLHq/LgUMVtkWeDKp9bjvsq5y2zItMewYYd0K1qsBzLzyn9TOiyTTbjw3zzp9s4Y4dJwA40DvK7duOc6Q/SiKdZfc0ExMtu+jeXd1orQs29Y4m02it+d5DB/nL/tJ792SymtGEWbn7XLalMxhN2imgyxuD9I0mGYmn7MnMobz9au/Z2c0ft3WVPH84nkZrYz+BfHEfjCT5pz/sKMs6ssbUVOWhxu8uuT0kwKdv3cHNP9ky6fnmChF3YcHjdTkIelwV9/KnQ6uZCV/O5O51eZufFGfKWFj98qrEzcuyaWLJDIfN7pNdXSNorfnQr57mQ7962q40S2XglIMlskcHotz0nb/yf9tzIhtNZvjF4x386x27+NEjh0u+32qXNGwZF6PJNNmsNnzyPFsGjJWqVuU+XNS9MhIrbS1ZFX6N323vWds/muT27V385K9HeOLw5HEE1jxAtd/N+cvrx83Kf/b4iJ0rNBHf//NBHjs4+3n7Iu7Cgsfrds5JG2Q5WJOq5VTuPreT1YuMdMjxLKWgeR6/2znm5pVvy3QOGuK+90SYe3Z2s61zmFRGs6vLWLJSKr2yHIZjKeoCbm5Yv5hnjw/z80eP2K91j8T57B+fBSiIX8jHqoqtCVWtDcEfiibtPQAscT/UF7F3uBrKE/e+0QRpc+VvqfEB1Prd1AU8OJRx/NbDRgxBObn3Vntmtc/NZasaOToQLZhjAMO6OdIfmXSyNpXJ8m937OLhcT7JVBIRd2HB43U57DbBk02b6buXO7n7i3dexD++eK3dqVGMJfql2kcDnpy4WyK2+0SY7zx4wN5kxNresFTufDkMx5K01Pj51uvPY92SGg70jtqvHeyNkExnaazyjonztbCq4iqv27Z4BiMpIsmMnd+/tD6AUqa4W5V73kbgfWaOTilhtT5Z1ASMlcP1QWOVqlWxHxuaPI7A6nEP+VxctspozfyPO3fzhbt229d0qC9C1py8jpe4yVh0j8TJ6sKFZrNFxVIhzWPPV0pllFI3VXaYgjB9Xr2pnbdfuuJkDwOAV25s4++uWYW7zBC0hiovb790xbiWkmXvlPokYD0XS6Vtn/jYUIwnjw5x/foWwIjBBeguw04ohdVmCMaEsRUhAdBpCueG9loiyUzJLpPR/MrdvFFZnzKsbhmf28mSGj/7ekbtSt8S7VgyQ8ScuLU6aO7Y3mWfw6rwrTE2Vnl59viwnT55bDDGb7Z2snlP6QnlXz1xlA/9ykjWrPa7WdEYpLXWz507TvDtzQfYaX7y2deTu6mNTFC9WzfZ1lNB3CkjFRJAKeUEvgDcXdkhCsLMeP7aRbzhomUnexgArG+r4cPPX12x81l2U6lPAgHbc8/SORi1LRyAt12yHMiJZE84Ma3MFGuBEBQKltupOD5k3DA2tNcA2EFg+VhiXeXL7UzVYYt7bon/isYgTx7J+eNWlZ7f+TIST7G3O8zf/vxJ3v+Lp8jmbVRuRTQvqvaxzfy0UuN3c3Qgymdve5Z3/HgL9+4cuz/qg3t7iaUyXLyygfa6AEop/uFFZ/Kx687A5VDc9vRxAPZ15yak8zc0L+a4uavUKVG5l5kKCfB+jIgCSYQUhDnCrtwnsGUiSaNyv2yVsZR/zeIQG5fWFUzAprOagTyrI5vVfPlPe/j9U51EEuN3lAzliXu+YDWHfGTMm8WG9jrAmHTtGIjy6u/8lVufOobW2u6Jr/a57E6ijgFDAPOz65c3BgomK4dK5KsPR1P88OFDgLGL02+e7KRvNIHT3JQd4FM3nMkFK+pZ0RjkyjOa2NY5bPfW//udYxfVHxuKc96yOn5x80X2J6Eb1rfw3qtO54rVTdz2zHGyWW1vjwil7SH7fHNYuU/JiBwvFVIp1Qq8HLgaOL9CYxMEYRKskLWJbJnOwRiJdJYLV9azv3eU157fjlKKlhofh/uj1AbcDEVTdI/E7XbBRw/1843795tneoZr1jTzg7cU/q+ttWY4auS2QGEff3O117Y+1reZlftAlKePDvH44QEePzzAaMLY9xWMlkqrBd+q3OsKKveqgp9tfeLoy8utPzIQ5XdPHeN1FyzlySOD/HZrJ8sbgjQEPTjMSYYzFoe45V0XA/CVe/aSNm9AG5fVcSDPWrE4PhRjjbkZSzEvO7eV+3b38PX79/FUxyCLqr10jyRKivtAJMkd27s4NhSjIeiZ9YRSmMKE6iSpkF8DPq61nnDVglLqZqXUFqXUlt7e3qmPVhCEAgJlVO6WZbC0PsC9H76Ct1xizD9YGTbrlhjim78A59anjhH0OPn+mzZxwfJ6njw6tmUwnsqSzGTtrpY2sxp1O3OVcrXPZS8g2nMizK+3dvCi9S2c01bDjx85nLNlvC57oZbVWWP55AArGnMTyk0hL8Nmi2O+LfPowX6S6SwvObuFNS0hjg/H6B1N2DesYqzxBjxONrTXFmwaAsaCrN5wgpbasdn9AC9a38Lz1y7ia/fuYziW4oPXGnbbUDTFI/v7CiaQ//D0MT596w7+9Gz3nFgyULlUyE3AL5VSh4GbgG8ppV5WfJCkQgpCZXE6FD536W4gn7ky19oEpa2usONmSY0hMlZlbbVDxlMZ7tx+guvWLebatYu46LQGhmIp22YBY4FSj9lhU2zLVHld9o2lwRTW9no//7e9i3A8zVsuWc5rL1jKvp5RHt7Xh0MZApvz3I2KPz9Wd7m5OhRgZWPQro7zbRlrD94ltX5aavycGI7TG07QOM6GJdYnjXWtNdT43SQzWRLpXH3aPZwouK5iHA7F116zgTddvIyfv+NCnr92EWAs6Pqb7z/GrXn711oblvdHknNiyUCFUiG11iu01su11suB3wB/q7W+taIjFQShJFVel73FXj4OU/itBUxtdYWiYlXuZy0xctKtyv3hfX2EE2l7h6j6gButjeX1AMl0lpf/1yO87UdPALkK29ig22hptOYCrInMC1c0UB/08LHrzmDTsjpefHYLfreTh/f3UeU1FphZ7+kNJ3A5VMEEcHt9AKdD4XQoljUEbFumN5wg5DU6bawM+MU1PpbU+khlNPt6wnamTDGWyJ7TVmN/asiv3i1baSIxDnpdfO7GdZy3rN7+PTzTYUzY/vzRo/Zx1uQyFNpXs0k5nruVCrldKWXttvspYClAfniYIAhzzz++eK290KeYgMfFQCRJU8g7ZiFUiylaS2r9NIe8dob6g3t78budXLSyHoB6s/oeiCRpqPLyo0cO2fvdAnYGDOQ+DViecr0p7p+4fg2fuH6NfVzI5+bzL1vH3c+e4Axzo3SnQ/Ei34sAAArxSURBVNkhYbUBd0H7p9vpoL3Oz2giQ13Qw5C5IrVv1KjME6kMo4k09UEPPreTFnMc8VR23K0G2+sDvOGipdx0XjvPHjcEeTSetm0cq9ou10ZxO40kTuumsOXIIHu7w6xeFLK7ZKZyvpkyqbhrrR8Gyl63rbV+y0wGJAjC1LAq7FJYXvyKhrHif8WqJq47axFrFoc4p72WpzqMnvc/7+vl4tMa7MA1q/oeiCSJpzJ8/b79VHlddpdJTV5Xy8vOXUI8lbM3GsapmgFuOq+Nm85rK3ju5stX8snfbS+Z3Lh6UYgTI3HDQkln+cAvn+aJwwO01wUYTaQ5Phynxfw0Yn0FaBrHc3c6FP/ysvVAbuen/Mq9yxTk/HNNRo3fTTSZoT7oYShqTKKuXhSiayjO+tYanj0+zJrFobLPNxNkhaogLGAs73t549gVrksbAvz3GzcR8Lg4b1kdh/oiPN0xxOH+KJebKzEh17UyEEny2CGjy+U9V55mv54/8Xnz5afxd9essucA6qe4Hd0rNxpi73aOrSc//7J1fON159oTuLc9c5zukQStdX67Y8eq2PMFebwJ1XysBVThRK7T5dhQnAbzk0C5WL+LMxaFWN4YZFfXCKlMlp5wnKvWNPPoJ6/heac1THKWyiDiLggLmJy4l7ZtLM5bZvSif+nu3QBctjrX8GBV3wPRJA/u6cXrcvCmi5fZffK1JTbhDti2zOTCmo/H5eC+j1zBXR+8fMxri6p9LGsI2u2fXpeDuz94Of/44rV2sJol6vVBjz2+csTd8tyP9Ed55bcfoXMwyvGh2JQtFGscbXV+zlxcza6ucC5yoMZHc7VvzgLsRNwFYQFjed/LS9gy+axvrcHlUPxlfz/nL69jZd7NwOqGGRhN8uDeHi5c2UDI52ad+Z5gydWxhZbOVDitqYrTmqrGfX1pvfEp5D9euZ4zFodorPLa+89abYtWHz8wrueejyXujxzoZ+uRQbYcHjTFvXxLBrA/QbTVBTizJcTRgagdTdAyR167hYi7ICxgLHtkMnH3uZ2c1VqDx+XgP155dkF16XU5CXldbD82zIHeCFeYVf2Vq5s4vbmqZCU6XVumHM5dWsfWT1/Ly8/N+fU1/sLK3XhsiOl43TL5WLbMoT5DiI8Px6ZVudf4c5X7msVGF9JmcyOUJVPw7ivBqRGVJwjCrOCfwHMv5rMvWUs4ni5ZNddXeXhon7Hw0Oqied/Vp/O+q08vea4VjUE8Lgcrmya+qUyXhiKrJWfL5MS4pdZXED0wEVaP/aFeI8p3z4kwkWTG7v4pl3xxbzM/YdxnivtcV+6TirtSqh34CbAYyALf1Vr/Z9Exrwc+bn47CrxHa/1MhccqCMIUaQ55Wd4QKCvy+NyldeO+VhfwcKQ/itflYLXZujiRd7yutYbdn3uhvex/tilVud+4oZXGKm9ZY/C6nHhcDjthcosZCTzdyr29PkBLjY9qn4vOwRjVeamXc0U5P81KhXxSKRUCtiql7tFa78w75hBwhdZ6UCl1PfBd4MJZGK8gCFPgIy84g/dccdrkB06C5Z2vXVJddlzxXAk7wIvObiGVydp+PMAVq5tsC6kcQl4X/WmjBfOY3eM+NSvlmjOb6QnHWWxOnL7p4uVsOzbMjecsmdJ5KkE5fe5dQJf5OKyUslIhd+Yd80jeWx4FCptXBUE4KVR5K1MxWlEA57TVzvhcs8Giah/vmuFNLORzFcQZwNTTG9e11vCvL19vf//R686Y0ZhmwpQmVMdLhSzi7cCd0x+SIAinGlblvr615iSPZPYo3oTc7VRltVGeqpR9S58kFdI65ioMcb90nNdvBm4GWLp06ZQHKwjCycHqejmnfeGKe8hr+OUhn4twPM3iGt+cWkuVpixxLyMVEqXU2cD3geu11iW39tZafxfDj2fTpk1T3/ZFEISTwks3LMGh1IT95/Mdq3Jf31rDIwf6p9wpc6pRkVRIpdRS4HfAG7XWeys7REEQTjYtNX7eefnKOVtdeTIImXMTVgTyXEXzzhaVSoX8J6ABI8cdIK213lT54QqCIMwO1irVs1trcTqU3ac+X6lIKqTW+h3AOyo1KEEQhLnGsmWW1gf42dsv5MyWuUlvnC1khaogCAJGxjxAY8hjWzPzGRF3QRAE4IZ1LcRTGRZXz20GzGwh4i4IgoCRb29tcr0QkFRIQRCEBYiIuyAIwgKknD73dqXUA0qpXUqpZ5VSHyhxjFJKfV0ptV8ptU0ptXF2hisIgiCUQ6VSIa8HVpn/LgS+jaRCCoIgnDQmrdy11l1a6yfNx2HASoXM50bgJ9rgUaBWKdVS8dEKgiAIZVGpVMhWoCPv+07G3gAEQRCEOaJscZ8kFbLUCtYxwWBKqZuVUluUUlt6e3unNlJBEAShbMoS9zJSITuB9rzv24DjxQdprb+rtd6ktd7U1FT+DimCIAjC1FBaT5y8a6ZC/hgY0Fp/cJxjXgS8D7gBYyL161rrCyY5by9wZDqDBhqBvmm+d74i17zwea5dL8g1T4dlWutJq+NyxP1S4M/AdowNsqEoFdK8AXwTeCEQBd6qtd4y/bFPMmiltjzXUiflmhc+z7XrBbnm2aRSqZAaeG+lBiUIgiDMDFmhKgiCsACZr+L+3ZM9gJOAXPPC57l2vSDXPGtM6rkLgiAI84/5WrkLgiAIEzDvxF0p9UKl1B4zpOwTJ3s8s4VS6rBSartS6mml1BbzuXql1D1KqX3m17qTPc7popT6oVKqRym1I++5kte3UILpxrnmzyqljpl/56eVUjfkvfZJ85r3KKWuOzmjnj7jhQ4u5L/zBNc8939nrfW8+Qc4gQPASsADPAOsPdnjmqVrPQw0Fj33ReAT5uNPAF842eOcwfVdDmwEdkx2fRjrJ+7E6Nq6CHjsZI+/gtf8WeCjJY5da/737QVWmP/dO0/2NUzxeluAjebjELDXvK4F+3ee4Jrn/O883yr3C4D9WuuDWusk8EuM0LLnCjdiLCjD/PqykziWGaG1fggYKHp6vOtbEMF041zzeNwI/FJrndBaHwL2Y/z3P2/Q44cOLti/8wTXPB6z9neeb+L+XAoo08CflFJblVI3m88t0lp3gfEfEdB80kY3O4x3fQv97/4+04b4YZ7VtqCuuSh08Dnxdy4RtDinf+f5Ju5lBZQtEC7RWm/EyMp/r1Lq8pM9oJPIQv67fxs4DdgAdAFfNp9fMNc8SehgwaElnlso1zznf+f5Ju5lBZQtBLTWx82vPcDvMT6qdVsfU82vPSdvhLPCeNe3YP/uWuturXVGa50FvkfuI/mCuOZxQgcX9N+51DWfjL/zfBP3J4BVSqkVSikP8FrgtpM8poqjlAqau16hlAoCLwB2YFzr/2/vjnESCoI4jH9b2WtlqWewsPAC0tlZQeExuIOdpRWFtfZewEoxhhjjIawtnsUsCSF5xoCwYfL9EkLBK+afCRN2Fx6jetkIuG9T4cb05XsAhvXbFKfA13xZv+uW9pQviD5DZL4speyVUo6Ifzl72nZ966j3nLoFZl3XXS+8lLbPfZmb9Ln16fIKp9ED4gT6Exi3rmdDGY+JE/QX4G2eEzgAHoGP+rzfutY1Mt4Ry9Nv4tPLVV8+Yul6U3v+Cpy0rv8fM09qpml9ox8uXD+umd+B89b1r5D3jNhimALP9THI3OdfMm+9z/5CVZIS2rVtGUnSHzjcJSkhh7skJeRwl6SEHO6SlJDDXZIScrhLUkIOd0lK6Af5RW4d5VVHcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(encoder, decoder, en_fr_trainloader, learning_rate=0.01, num_epochs=3, print_every=1000, plot_every=100, \n",
    "      max_seq_length=10, teacher_forcing_ratio=0.5, use_attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (axiom)",
   "language": "python",
   "name": "axiom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
